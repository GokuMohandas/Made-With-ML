{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "14_Advanced_RNNs",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "bOChJSNXtC9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Advanced RNNs"
      ]
    },
    {
      "metadata": {
        "id": "OLIxEDq6VhvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/images/logo.png\" width=150>\n",
        "\n",
        "In this notebook we're going to cover some advanced topics related to RNNs.\n",
        "\n",
        "1. Conditioned hidden state\n",
        "2. Char-level embeddings\n",
        "3. Encoder and decoder\n",
        "4. Attentional mechanisms\n",
        "5. Implementation\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "41r7MWJnY0m8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Set up"
      ]
    },
    {
      "metadata": {
        "id": "EJDhjHCHY0_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 install http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl\n",
        "!pip3 install torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p0FbOd6IZmzX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from argparse import Namespace\n",
        "import collections\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOsqAo4XZpXQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Set Numpy and PyTorch seeds\n",
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QHfvEzQ9ZweF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VoMq0eFRvugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conditioned RNNs"
      ]
    },
    {
      "metadata": {
        "id": "ZUsj7HjBp69f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conditioning an RNN is to add extra information that will be helpful towards a prediction. We can encode (embed it) this information and feed it along with the sequential input into our model. For example, suppose in our document classificaiton example in the previous notebook, we knew the publisher of each news article (NYTimes, ESPN, etc.). We could have encoded that information to help with the prediction. There are several different ways of creating a conditioned RNN.\n",
        "\n",
        "**Note**: If the conditioning information is novel for each input in the sequence, just concatenate it along with each time step's input."
      ]
    },
    {
      "metadata": {
        "id": "Kc8H9JySmtLa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Make the initial hidden state the encoded information instead of using the initial zerod hidden state. Make sure that the size of the encoded information is the same as the hidden state for the RNN.\n"
      ]
    },
    {
      "metadata": {
        "id": "pKlb9SjfpbED",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**IMAGE with only h_0 being the condition embedding.**"
      ]
    },
    {
      "metadata": {
        "id": "jbrlQHx2x8Aa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Oep0xbKYoNiN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    batch_size=4,\n",
        "    condition_vocab_size=3, # vocabulary for condition possibilities\n",
        "    embedding_dim=100,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFoiV-fqmvRo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9840cfe7-552c-4b53-dff3-e9765b643149"
      },
      "cell_type": "code",
      "source": [
        "# Initialize hidden state\n",
        "condition = torch.LongTensor([0, 2, 1, 2]) # batch size of 4 with a vocab size of 3\n",
        "condition_embeddings = nn.Embedding(embedding_dim=args.embedding_dim, # should be same as RNN hidden dim\n",
        "                                    num_embeddings=args.condition_vocab_size, # of unique conditions\n",
        "                                   )\n",
        "num_directions = 1\n",
        "if args.bidirectional:\n",
        "    num_directions = 2\n",
        "hidden_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
        "    args.num_layers * num_directions, 1, 1) # initial state to RNN\n",
        "print (hidden_t.size())\n",
        "\n",
        "# Feed into RNN\n",
        "# y_out, _ = self.rnn(x_embedded, hidden_t)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "REgyaMDgmtHw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2. Concatenate the encoded information with the hidden state at each time step. Do not replace the hidden state because the RNN needs that to learn. "
      ]
    },
    {
      "metadata": {
        "id": "yUIg5o-dpiZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**IMAGE with all hidden states being concatenated with condition embedding.**"
      ]
    },
    {
      "metadata": {
        "id": "eQ-h28o-pi4X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf29a096-c2b7-4654-9754-39424130b114"
      },
      "cell_type": "code",
      "source": [
        "# Initialize hidden state\n",
        "hidden_t = torch.zeros((args.num_layers * num_directions, args.batch_size, args.hidden_dim))\n",
        "print (hidden_t.size())"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2Z6hYSIdqBQ4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def concat_condition(condition_embeddings, condition, hidden_t, num_layers, num_directions):\n",
        "    condition_t = condition_embeddings(condition).unsqueeze(0).repeat(\n",
        "        num_layers * num_directions, 1, 1)\n",
        "    hidden_t = torch.cat([hidden_t, condition_t], 2)\n",
        "    return hidden_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tjyzq_s5pixL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b855d7f5-3fa2-4afa-91ba-017f7a7def76"
      },
      "cell_type": "code",
      "source": [
        "# Loop through the inputs time steps\n",
        "hiddens = []\n",
        "seq_size = 1\n",
        "for t in range(seq_size):\n",
        "    hidden_t = concat_condition(condition_embeddings, condition, hidden_t, \n",
        "                                args.num_layers, num_directions)\n",
        "    print (hidden_t.size())\n",
        "    \n",
        "    # Feed into RNN\n",
        "    # hidden_t = rnn_cell(x_in[t], hidden_t)\n",
        "    ..."
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 4, 200])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A-0_81jMXg_J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Char-level embeddings"
      ]
    },
    {
      "metadata": {
        "id": "w0yUKKpq3pu_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2D conv operations where inputs are words in a sentence represented at the character level|  $\\in \\mathbb{R}^{NXSXWXE}$  and outputs are embeddings for each word (based on convlutions applied at the character level.) This is the more typical way CNNs are used on text. Chararacter-level embeddings create representations that map words at a character level. Ex. \"toy\" and \"toys\" will be close to each other."
      ]
    },
    {
      "metadata": {
        "id": "QOdIvz0G3O8C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Arguments\n",
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    batch_size=64,\n",
        "    vocab_size=20, # vocabulary\n",
        "    seq_size=10, # max length of each sentence\n",
        "    word_size=15, # max length of each word\n",
        "    embedding_dim=100,\n",
        "    num_filters=100, # filters per size\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "raztXIeYXYJT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, num_input_channels, \n",
        "                 num_output_channels, padding_idx):\n",
        "        super(Model, self).__init__()\n",
        "        \n",
        "        # Char-level embedding\n",
        "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                       num_embeddings=num_embeddings,\n",
        "                                       padding_idx=padding_idx)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList(\n",
        "            [nn.Conv2d(num_input_channels, num_output_channels, (1, f)) \\\n",
        "             for f in [2, 3, 4]])\n",
        "\n",
        "    def forward(self, x, apply_softmax=False):\n",
        "        \n",
        "        # x: (N, seq_len, word_len)\n",
        "        input_shape = x.size()\n",
        "        word_len = x.size(2)\n",
        "        x = x.view(-1, word_len) # (N*seq_len, word_len)\n",
        "        \n",
        "        # Embedding\n",
        "        x = self.embeddings(x) # (N*seq_len, word_len, embedding_dim)\n",
        "        x = x.view(*input_shape, -1) # (N, seq_len, word_len, embedding_dim)\n",
        "        \n",
        "        # Sum character level embeddings for each word\n",
        "        x = x.sum(2) # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # insert channel dim\n",
        "        x = x.unsqueeze(1) # (N, 1, seq_len, embedding_dim)\n",
        "        \n",
        "        # CNN\n",
        "        ### (N, embedding_dim, seq_len, (embedding_dim - filter_size + 1))\n",
        "        z = [F.relu(conv(x)) for conv in self.conv]\n",
        "        ### (N, seq_len, (embedding_dim - filter_size + 1), embedding_dim)\n",
        "        z = [zz.view((zz.size(0), zz.size(2), zz.size(3), zz.size(1))) \\\n",
        "             for zz in z]\n",
        "        \n",
        "        # Pooling\n",
        "        z = [torch.sum(zz, 2) for zz in z] # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # Concat to get char-level embeddings\n",
        "        z = torch.cat(z, 2) # join conv outputs\n",
        "        \n",
        "        return z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MzHVs8Xe0Zph",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce477db2-068a-4e24-8eb6-f90ff8340208"
      },
      "cell_type": "code",
      "source": [
        "# Input\n",
        "input_size = (args.batch_size, args.seq_size, args.word_size)\n",
        "x_in = torch.randint(low=0, high=args.vocab_size, size=input_size).long()\n",
        "print (x_in.size())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 15])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0B_Xscby2PMQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "62c0488e-4396-468d-fe17-4f0c1fcec86a"
      },
      "cell_type": "code",
      "source": [
        "# Initial char-level embedding model\n",
        "model = Model(embedding_dim=args.embedding_dim, \n",
        "              num_embeddings=args.vocab_size, \n",
        "              num_input_channels=1, \n",
        "              num_output_channels=args.num_filters,\n",
        "              padding_idx=0)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_modules of Model(\n",
            "  (embeddings): Embedding(20, 100, padding_idx=0)\n",
            "  (conv): ModuleList(\n",
            "    (0): Conv2d(1, 100, kernel_size=(1, 2), stride=(1, 1))\n",
            "    (1): Conv2d(1, 100, kernel_size=(1, 3), stride=(1, 1))\n",
            "    (2): Conv2d(1, 100, kernel_size=(1, 4), stride=(1, 1))\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8DIgeEZFXYR2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ebac6d6e-9edf-4c56-a38c-473fd3ee3748"
      },
      "cell_type": "code",
      "source": [
        "# Forward pass to get char-level embeddings\n",
        "z = model(x_in)\n",
        "print (z.size())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([64, 10, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nzTscaE10HFA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are several different ways you can use these char-level embeddings:\n",
        "\n",
        "1. Concat char-level embeddings with word-level embeddings, since we have an embedding for each word (at a char-level) and then feed it into an RNN. \n",
        "2. You can feed the char-level embeddings into an RNN to processes them."
      ]
    },
    {
      "metadata": {
        "id": "nyCQ13_ckV_c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Encoder and decoder"
      ]
    },
    {
      "metadata": {
        "id": "_sixbu74kbJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "So far we've used RNNs to `encode` a sequential input and generate hidden states. We use these hidden states to `decode` the predictions. So far, the encoder was an RNN and the decoder was just a few fully connected layers followed by a softmax layer (for classification). But the encoder and decoder can assume other architectures as well. For example, the decoder could be an RNN that processes the hidden state outputs from the encoder RNN. "
      ]
    },
    {
      "metadata": {
        "id": "ucOEip8epWma",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**IMAGE OF ENCODER AND DECODER**"
      ]
    },
    {
      "metadata": {
        "id": "p_OJFyY97bF_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, hidden_dim, \n",
        "                 num_layers, bidirectional, padding_idx=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        # Embeddings\n",
        "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_embeddings,\n",
        "                                            padding_idx=padding_idx)\n",
        "        \n",
        "        # GRU weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, \n",
        "                          num_layers=num_layers, batch_first=True, \n",
        "                          bidirectional=bidirectional)\n",
        "\n",
        "    def forward(self, x_in, x_lengths):\n",
        "        \n",
        "        # Word level embeddings\n",
        "        z_word = self.word_embeddings(x_in)\n",
        "   \n",
        "        # Feed into RNN\n",
        "        out, h_n = self.gru(z)\n",
        "        \n",
        "        # Gather the last relevant hidden state\n",
        "        out = gather_last_relevant_hidden(out, x_lengths)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRXtaGPlpyH7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, dropout_p):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, encoder_output, apply_softmax=False):\n",
        "        \n",
        "        # FC layers\n",
        "        z = self.dropout(encoder_output)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SnKyCPVj-OVi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_embeddings, hidden_dim, num_layers, \n",
        "                 bidirectional, output_dim, dropout_p, padding_idx=0):\n",
        "        super(Model, self).__init__()\n",
        "        self.encoder = Encoder(embedding_dim, num_embeddings, hidden_dim, \n",
        "                               num_layers, bidirectional, padding_idx=0)\n",
        "        self.decoder = Decoder(hidden_dim, output_dim, dropout_p)\n",
        "        \n",
        "    def forward(self, x_in, x_lengths, apply_softmax=False):\n",
        "        encoder_outputs = self.encoder(x_in, x_lengths)\n",
        "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfeoErsc-Tum",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "7252d6f2-e3e6-43b0-cc1c-cb6da15428a7"
      },
      "cell_type": "code",
      "source": [
        "model = Model(embedding_dim=100, num_embeddings=1000, hidden_dim=100, \n",
        "              num_layers=1, bidirectional=False, output_dim=4, \n",
        "              dropout_p=0.1, padding_idx=0)\n",
        "print (model.named_parameters)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of Model(\n",
            "  (encoder): Encoder(\n",
            "    (word_embeddings): Embedding(1000, 100, padding_idx=0)\n",
            "    (gru): GRU(100, 100, batch_first=True)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (dropout): Dropout(p=0.1)\n",
            "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LAsOI6jEmTd0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Attentional mechanisms"
      ]
    },
    {
      "metadata": {
        "id": "vJN5ft5Sc_kb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When processing an input sequence with an RNN, recall that at each time step we process the input and the hidden state at that time step. For many use cases, it's advantageous to have access to the inputs at all time steps and pay selective attention to the them at each time step. For example, in machine translation, it's advantageous to have access to all the words when translating to another language because translations aren't necessarily word for word. "
      ]
    },
    {
      "metadata": {
        "id": "mNkayU0rf-ua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Attention can sound a bit confusing so let's see what happens at each time step. At time step j, the model has processed inputs $x_0, x_1, x_2, ..., x_j$ and has generted hidden states $h_0, h_1, h_2, ..., h_j$. The idea is to use all the processed hidden states to make the prediction and not just the last one. There are several approaches to how we can do this.\n",
        "\n",
        "With **soft attention**, we learn a vector of floating points (probabilities) to multiply with the hidden states to create the context vector.\n",
        "\n",
        "Ex. [0.1, 0.3, 0.1, 0.4, 0.1]\n",
        "\n",
        "With **hard attention**, we can learn a binary vector to multiply with the hidden states to create the context vector. \n",
        "\n",
        "Ex. [0, 0, 0, 1, 0]"
      ]
    },
    {
      "metadata": {
        "id": "9rpAC-m6vAIE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**IMAGE OF ATTENTIONAL MECHANISM**"
      ]
    },
    {
      "metadata": {
        "id": "gYSIAVQqu3Ab",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're going to focus on soft attention because it's more widley used and we can visualize how much of each hidden state helps with the prediction, which is great for interpretability. "
      ]
    },
    {
      "metadata": {
        "id": "9Ch21nZNvDHO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**IMAGE SOFT ATTENTION DIAGRAM**"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "0iNnQzdxnGvn"
      },
      "cell_type": "markdown",
      "source": [
        "# Document classification with RNNs"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "n38ZJoVZnGaE"
      },
      "cell_type": "markdown",
      "source": [
        "We're going to implement the same document classification task as in the previous notebook but we're going to use the concepts we've learned in this notebook. Specifically, we will represent each word at the char-level using a CNN and at the word-level. We will separate the encoder and the decoder and use an attentional interface on top of the encoder.\n",
        "\n",
        "**Why not machine translation?** Normally, machine translation is the go-to example for demonstrating attention but it's not really practical. How many situations can you think of that require a seq to generate another sequence? Instead we're going to apply attention with our document classification example to see which input tokens are more influential towards predicting the genre."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Fu7HgEqbnGFY"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "elL6BxtCmNGf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from argparse import Namespace\n",
        "import collections\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ddOpCCpAmM2B",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_seeds(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        \n",
        "# Creating directories\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "8bce4440-3f74-4f34-9478-61cc903109b8",
        "id": "TTwkuoZdmMlF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "args = Namespace(\n",
        "    seed=1234,\n",
        "    cuda=False,\n",
        "    shuffle=True,\n",
        "    data_file=\"news.csv\",\n",
        "    split_data_file=\"split_news.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"news\",\n",
        "    reload_from_files=False,\n",
        "    train_size=0.7,\n",
        "    val_size=0.15,\n",
        "    test_size=0.15,\n",
        "    pretrained_embeddings=None,\n",
        "    cutoff=25, # token must appear at least <cutoff> times to be in SequenceVocabulary\n",
        "    num_epochs=5,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=64,\n",
        "    embedding_dim=100,\n",
        "    num_filters=100,\n",
        "    hidden_dim=100,\n",
        "    num_layers=1,\n",
        "    bidirectional=False,\n",
        "    dropout_p=0.1,\n",
        ")\n",
        "\n",
        "# Set seeds\n",
        "set_seeds(seed=args.seed, cuda=args.cuda)\n",
        "\n",
        "# Create save dir\n",
        "handle_dirs(args.save_dir)\n",
        "\n",
        "# Expand filepaths\n",
        "args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "print(\"Expanded filepaths: \")\n",
        "print(\"\\t{}\".format(args.vectorizer_file))\n",
        "print(\"\\t{}\".format(args.model_state_file))\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "print(\"Using CUDA: {}\".format(args.cuda))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Expanded filepaths: \n",
            "\tnews/vectorizer.json\n",
            "\tnews/model.pth\n",
            "Using CUDA: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "xfiWhgX5mMQ5"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "baAsxXNFmMCF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3tJi_HyOmLw-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/GokuMohandas/practicalAI/master/data/news.csv\"\n",
        "response = urllib.request.urlopen(url)\n",
        "html = response.read()\n",
        "with open(args.data_file, 'wb') as fp:\n",
        "    fp.write(html)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "661ecfc5-b968-4487-9cbb-c1459e3efdad",
        "id": "wrI_df4bmLjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(args.data_file, header=0)\n",
        "df.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Business</td>\n",
              "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Business</td>\n",
              "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Business</td>\n",
              "      <td>Oil prices soar to all-time record, posing new...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category                                              title\n",
              "0  Business  Wall St. Bears Claw Back Into the Black (Reuters)\n",
              "1  Business  Carlyle Looks Toward Commercial Aerospace (Reu...\n",
              "2  Business    Oil and Economy Cloud Stocks' Outlook (Reuters)\n",
              "3  Business  Iraq Halts Oil Exports from Main Southern Pipe...\n",
              "4  Business  Oil prices soar to all-time record, posing new..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "cc3ed9a2-fb55-4453-e1ef-734f4f4884fa",
        "id": "TreK7nqEmLTN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "by_category = collections.defaultdict(list)\n",
        "for _, row in df.iterrows():\n",
        "    by_category[row.category].append(row.to_dict())\n",
        "for category in by_category:\n",
        "    print (\"{0}: {1}\".format(category, len(by_category[category])))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Business: 30000\n",
            "Sci/Tech: 30000\n",
            "Sports: 30000\n",
            "World: 30000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "35nb3LxLmLCA",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "final_list = []\n",
        "for _, item_list in sorted(by_category.items()):\n",
        "    if args.shuffle:\n",
        "        np.random.shuffle(item_list)\n",
        "    n = len(item_list)\n",
        "    n_train = int(args.train_size*n)\n",
        "    n_val = int(args.val_size*n)\n",
        "    n_test = int(args.test_size*n)\n",
        "\n",
        "  # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "    for item in item_list[n_train:n_train+n_val]:\n",
        "        item['split'] = 'val'\n",
        "    for item in item_list[n_train+n_val:]:\n",
        "        item['split'] = 'test'  \n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "48036a8c-cf80-4702-fdb2-8fd506fff475",
        "id": "Y48IvuSfmK07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "split_df = pd.DataFrame(final_list)\n",
        "split_df[\"split\"].value_counts()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "train    84000\n",
              "val      18000\n",
              "test     18000\n",
              "Name: split, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RWuNBxAXmKk2",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = ' '.join(word.lower() for word in text.split(\" \"))\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "    \n",
        "split_df.title = split_df.title.apply(preprocess_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "df418e24-d70d-4599-d822-e66ecfc06419",
        "id": "fG9n77eLmKWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "split_df.to_csv(args.split_data_file, index=False)\n",
        "split_df.head()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>split</th>\n",
              "      <th>title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>general electric posts higher rd quarter profit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>lilly to eliminate up to us jobs</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>s amp p lowers america west outlook to negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>does rand walk the talk on labor policy ?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Business</td>\n",
              "      <td>train</td>\n",
              "      <td>housekeeper advocates for changes</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   category  split                                            title\n",
              "0  Business  train  general electric posts higher rd quarter profit\n",
              "1  Business  train                 lilly to eliminate up to us jobs\n",
              "2  Business  train  s amp p lowers america west outlook to negative\n",
              "3  Business  train        does rand walk the talk on labor policy ?\n",
              "4  Business  train                housekeeper advocates for changes"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "m-a0OpqhmKJc"
      },
      "cell_type": "markdown",
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RUMQ_MwumJ8F",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Vocabulary(object):\n",
        "    def __init__(self, token_to_idx=None):\n",
        "\n",
        "        # Token to index\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self.token_to_idx = token_to_idx\n",
        "\n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'token_to_idx': self.token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token in self.token_to_idx:\n",
        "            index = self.token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self.token_to_idx)\n",
        "            self.token_to_idx[token] = index\n",
        "            self.idx_to_token[index] = token\n",
        "        return index\n",
        "\n",
        "    def add_tokens(self, tokens):\n",
        "        return [self.add_token[token] for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1LtYf3lpExBb",
        "colab_type": "code",
        "outputId": "d51874b0-c06b-4384-bb67-752b7fca69e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Vocabulary instance\n",
        "category_vocab = Vocabulary()\n",
        "for index, row in df.iterrows():\n",
        "    category_vocab.add_token(row.category)\n",
        "print (category_vocab) # __str__\n",
        "print (len(category_vocab)) # __len__\n",
        "print (category_vocab.lookup_token(\"Business\"))\n",
        "print (category_vocab.lookup_index(0))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Vocabulary(size=4)>\n",
            "4\n",
            "0\n",
            "Business\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0zkF6CsE_yH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sequence vocabulary"
      ]
    },
    {
      "metadata": {
        "id": "QtntaISyE_1c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Next, we're going to create our Vocabulary classes for the article's title, which is a sequence of words."
      ]
    },
    {
      "metadata": {
        "id": "ovI8QRefEw_p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4W3ZouuTEw1_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self.mask_token = mask_token\n",
        "        self.unk_token = unk_token\n",
        "        self.begin_seq_token = begin_seq_token\n",
        "        self.end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self.mask_token)\n",
        "        self.unk_index = self.add_token(self.unk_token)\n",
        "        self.begin_seq_index = self.add_token(self.begin_seq_token)\n",
        "        self.end_seq_index = self.add_token(self.end_seq_token)\n",
        "        \n",
        "        # Index to token\n",
        "        self.idx_to_token = {idx: token \\\n",
        "                             for token, idx in self.token_to_idx.items()}\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self.unk_token,\n",
        "                         'mask_token': self.mask_token,\n",
        "                         'begin_seq_token': self.begin_seq_token,\n",
        "                         'end_seq_token': self.end_seq_token})\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        return self.token_to_idx.get(token, self.unk_index)\n",
        "    \n",
        "    def lookup_index(self, index):\n",
        "        if index not in self.idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the SequenceVocabulary\" % index)\n",
        "        return self.idx_to_token[index]\n",
        "    \n",
        "    def __str__(self):\n",
        "        return \"<SequenceVocabulary(size=%d)>\" % len(self.token_to_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_idx)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g5UHjpi3El37",
        "colab_type": "code",
        "outputId": "ac3b2630-0dd1-4f85-e337-9ce056df4724",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Get word counts\n",
        "word_counts = Counter()\n",
        "for title in split_df.title:\n",
        "    for token in title.split(\" \"):\n",
        "        if token not in string.punctuation:\n",
        "            word_counts[token] += 1\n",
        "\n",
        "# Create SequenceVocabulary instance\n",
        "title_word_vocab = SequenceVocabulary()\n",
        "for word, word_count in word_counts.items():\n",
        "    if word_count >= args.cutoff:\n",
        "        title_word_vocab.add_token(word)\n",
        "print (title_word_vocab) # __str__\n",
        "print (len(title_word_vocab)) # __len__\n",
        "print (title_word_vocab.lookup_token(\"general\"))\n",
        "print (title_word_vocab.lookup_index(805))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=4400)>\n",
            "4400\n",
            "4\n",
            "measures\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1_wja0EfQNpA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We're also going to create an instance fo SequenceVocabulary that processes the input on a character level."
      ]
    },
    {
      "metadata": {
        "id": "5SpfS0BXP9pz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7ef21c8e-8349-4f74-88b8-af8597c67947"
      },
      "cell_type": "code",
      "source": [
        "# Create SequenceVocabulary instance\n",
        "title_char_vocab = SequenceVocabulary()\n",
        "for title in split_df.title:\n",
        "    for token in title:\n",
        "        title_char_vocab.add_token(token)\n",
        "print (title_char_vocab) # __str__\n",
        "print (len(title_char_vocab)) # __len__\n",
        "index = title_char_vocab.lookup_token(\"g\")\n",
        "print (index)\n",
        "print (title_char_vocab.lookup_index(index))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=35)>\n",
            "35\n",
            "4\n",
            "g\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Dag6H0SFHAG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Vectorizer"
      ]
    },
    {
      "metadata": {
        "id": "VQIfxcUuKwzz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Something new that we introduce in this Vectorizer is calculating the length of our input sequence. We will use this later on to extract the last relevant hidden state for each input sequence."
      ]
    },
    {
      "metadata": {
        "id": "tsNtEnhBEl6s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NewsVectorizer(object):\n",
        "    def __init__(self, title_word_vocab, title_char_vocab, category_vocab):\n",
        "        self.title_word_vocab = title_word_vocab\n",
        "        self.title_char_vocab = title_char_vocab\n",
        "        self.category_vocab = category_vocab\n",
        "\n",
        "    def vectorize(self, title, title_length=-1, word_length=-1):\n",
        "        \n",
        "        # Word-level vectorization\n",
        "        word_indices = [self.title_word_vocab.lookup_token(token) for token in title.split(\" \")]\n",
        "        word_indices = [self.title_word_vocab.begin_seq_index] + word_indices + \\\n",
        "            [self.title_word_vocab.end_seq_index]\n",
        "        if title_length < 0:\n",
        "            title_length = len(word_indices)\n",
        "        word_vector = np.zeros(title_length, dtype=np.int64)\n",
        "        word_vector[:len(word_indices)] = word_indices\n",
        "        word_vector[len(word_indices):] = self.title_word_vocab.mask_index\n",
        "        \n",
        "        # Char-level vectorization\n",
        "        if word_length < 0:\n",
        "            word_length = max([len(word) for word in title.split(\" \")])\n",
        "        char_vector = np.zeros((title_length, word_length), dtype=np.int64)\n",
        "        char_vector[0, :] = self.title_word_vocab.mask_index # <BEGIN>\n",
        "        char_vector[-1, :] = self.title_word_vocab.mask_index # <END>\n",
        "        for i, word in enumerate(title.split(\" \")):\n",
        "            char_vector[i+1,:len(word)] = [title_char_vocab.lookup_token(char) \\\n",
        "                                           for char in word] # i+1 b/c of <BEGIN> token\n",
        "            char_vector[i+1, len(word):] = self.title_char_vocab.mask_index\n",
        "                \n",
        "        return word_vector, char_vector, len(word_indices)\n",
        "    \n",
        "    def unvectorize_word_vector(self, word_vector):\n",
        "        tokens = [self.title_word_vocab.lookup_index(index) for index in word_vector]\n",
        "        title = \" \".join(token for token in tokens)\n",
        "        return title\n",
        "    \n",
        "    def unvectorize_char_vector(self, char_vector):\n",
        "        title = \"\"\n",
        "        for word_vector in char_vector:\n",
        "            for index in word_vector:\n",
        "                if index == self.title_char_vocab.mask_index:\n",
        "                    break\n",
        "                title += self.title_char_vocab.lookup_index(index)\n",
        "            title += \" \"\n",
        "        return title\n",
        "    \n",
        "    @classmethod\n",
        "    def from_dataframe(cls, df, cutoff=25):\n",
        "        \n",
        "        # Create class vocab\n",
        "        category_vocab = Vocabulary()        \n",
        "        for category in sorted(set(df.category)):\n",
        "            category_vocab.add_token(category)\n",
        "\n",
        "        # Get word counts\n",
        "        word_counts = Counter()\n",
        "        for title in df.title:\n",
        "            for token in title.split(\" \"):\n",
        "                word_counts[token] += 1\n",
        "        \n",
        "        # Create title vocab (word level)\n",
        "        title_word_vocab = SequenceVocabulary()\n",
        "        for word, word_count in word_counts.items():\n",
        "            if word_count >= cutoff:\n",
        "                title_word_vocab.add_token(word)\n",
        "                \n",
        "        # Create title vocab (char level)\n",
        "        title_char_vocab = SequenceVocabulary()\n",
        "        for title in df.title:\n",
        "            for token in title:\n",
        "                title_char_vocab.add_token(token)\n",
        "        \n",
        "        return cls(title_word_vocab, title_char_vocab, category_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        title_word_vocab = SequenceVocabulary.from_serializable(contents['title_word_vocab'])\n",
        "        title_char_vocab = SequenceVocabulary.from_serializable(contents['title_char_vocab'])\n",
        "        category_vocab = Vocabulary.from_serializable(contents['category_vocab'])\n",
        "        return cls(title_word_vocab=title_word_vocab, \n",
        "                   title_char_vocab=title_char_vocab, \n",
        "                   category_vocab=category_vocab)\n",
        "    \n",
        "    def to_serializable(self):\n",
        "        return {'title_word_vocab': self.title_word_vocab.to_serializable(),\n",
        "                'title_char_vocab': self.title_char_vocab.to_serializable(),\n",
        "                'category_vocab': self.category_vocab.to_serializable()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JtRRXU53El9Y",
        "colab_type": "code",
        "outputId": "c1cb0837-9907-4c99-e930-fd616355dfe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "cell_type": "code",
      "source": [
        "# Vectorizer instance\n",
        "vectorizer = NewsVectorizer.from_dataframe(split_df)\n",
        "print (vectorizer.title_word_vocab)\n",
        "print (vectorizer.title_char_vocab)\n",
        "print (vectorizer.category_vocab)\n",
        "word_vector, char_vector, title_length = vectorizer.vectorize(preprocess_text(\n",
        "    \"Roger Federer wins the Wimbledon tennis tournament.\"))\n",
        "print (\"word_vector:\", np.shape(word_vector))\n",
        "print (\"char_vector:\", np.shape(char_vector))\n",
        "print (\"title_length:\", title_length)\n",
        "print (word_vector)\n",
        "print (char_vector)\n",
        "print (vectorizer.unvectorize_word_vector(word_vector))\n",
        "print (vectorizer.unvectorize_char_vector(char_vector))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<SequenceVocabulary(size=4404)>\n",
            "<SequenceVocabulary(size=35)>\n",
            "<Vocabulary(size=4)>\n",
            "word_vector: (10,)\n",
            "char_vector: (10, 10)\n",
            "title_length: 10\n",
            "[   2    1 4151 1231   25    1 2392 4076   38    3]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0]\n",
            " [ 7 15  4  5  7  0  0  0  0  0]\n",
            " [21  5 18  5  7  5  7  0  0  0]\n",
            " [26 13  6 16  0  0  0  0  0  0]\n",
            " [12 17  5  0  0  0  0  0  0  0]\n",
            " [26 13 23 25  9  5 18 15  6  0]\n",
            " [12  5  6  6 13 16  0  0  0  0]\n",
            " [12 15 20  7  6  8 23  5  6 12]\n",
            " [30  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0]]\n",
            "<BEGIN> <UNK> federer wins the <UNK> tennis tournament . <END>\n",
            " roger federer wins the wimbledon tennis tournament .  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uk_QvpVfFM0S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ]
    },
    {
      "metadata": {
        "id": "oU7oDdelFMR9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pB7FHmiSFMXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NewsDataset(Dataset):\n",
        "    def __init__(self, df, vectorizer):\n",
        "        self.df = df\n",
        "        self.vectorizer = vectorizer\n",
        "        \n",
        "        def get_max_word_length(title):\n",
        "            words = [word for word in title.split(\" \")]\n",
        "            max_word_length = max([len(word) for word in words])\n",
        "            return max_word_length\n",
        "        \n",
        "        # Max lengths\n",
        "        get_title_length = lambda title: len(title.split(\" \"))\n",
        "        get_word_length = lambda title: get_max_word_length(title)\n",
        "        self.max_title_length = max(map(get_title_length, df.title)) + 2 # (<BEGIN> + <END>)\n",
        "        self.max_word_length = max(map(get_word_length, df.title))\n",
        "\n",
        "        # Data splits\n",
        "        self.train_df = self.df[self.df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "        self.val_df = self.df[self.df.split=='val']\n",
        "        self.val_size = len(self.val_df)\n",
        "        self.test_df = self.df[self.df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "        self.lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                            'val': (self.val_df, self.val_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "        self.set_split('train')\n",
        "\n",
        "        # Class weights (for imbalances)\n",
        "        class_counts = df.category.value_counts().to_dict()\n",
        "        def sort_key(item):\n",
        "            return self.vectorizer.category_vocab.lookup_token(item[0])\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, split_data_file):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        train_df = df[df.split=='train']\n",
        "        return cls(df, NewsVectorizer.from_dataframe(train_df))\n",
        "\n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, split_data_file, vectorizer_filepath):\n",
        "        df = pd.read_csv(split_data_file, header=0)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(df, vectorizer)\n",
        "\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return NewsVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self.vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self.target_split = split\n",
        "        self.target_df, self.target_size = self.lookup_dict[split]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Dataset(split={0}, size={1})\".format(\n",
        "            self.target_split, self.target_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.target_df.iloc[index]\n",
        "        title_word_vector, title_char_vector, title_length = self.vectorizer.vectorize(\n",
        "            row.title, title_length=self.max_title_length, \n",
        "            word_length=self.max_word_length)\n",
        "        category_index = self.vectorizer.category_vocab.lookup_token(row.category)\n",
        "        return {'title_word_vector': title_word_vector, \n",
        "                'title_char_vector': title_char_vector, \n",
        "                'title_length': title_length, \n",
        "                'category': category_index}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    def generate_batches(self, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "        dataloader = DataLoader(dataset=self, batch_size=batch_size, \n",
        "                                shuffle=shuffle, drop_last=drop_last)\n",
        "        for data_dict in dataloader:\n",
        "            out_data_dict = {}\n",
        "            for name, tensor in data_dict.items():\n",
        "                out_data_dict[name] = data_dict[name].to(device)\n",
        "            yield out_data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Dpb6ZHJFMeb",
        "colab_type": "code",
        "outputId": "19515dbf-4f86-4410-9578-497e79fc287d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        }
      },
      "cell_type": "code",
      "source": [
        "# Dataset instance\n",
        "dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "print (dataset) # __str__\n",
        "input_ = dataset[10] # __getitem__\n",
        "print (input_['title_word_vector'])\n",
        "print (input_['title_char_vector'])\n",
        "print (input_['title_length'])\n",
        "print (input_['category'])\n",
        "print (dataset.vectorizer.unvectorize_word_vector(input_['title_word_vector']))\n",
        "print (dataset.vectorizer.unvectorize_char_vector(input_['title_char_vector']))\n",
        "print (dataset.class_weights)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<Dataset(split=train, size=84000)\n",
            "[ 2 51  1 52 53 26 54  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0]\n",
            "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [18  5  9 12  8  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [18 15 18  4  5 16  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [25  8  6 27  7 20 14 12 11 22  0  0  0  0  0  0  0  0  0]\n",
            " [26 13 12 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 9  8 25 15  7  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [18  5  8  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
            " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
            "8\n",
            "0\n",
            "<BEGIN> delta <UNK> bankruptcy with labor deal <END> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK> <MASK>\n",
            " delta dodges bankruptcy with labor deal                      \n",
            "tensor([0.0000, 0.0000, 0.0000, 0.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_IUIqtbvFUAG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "xJV5WlDiFVVz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "embed  encoder  attend  predict"
      ]
    },
    {
      "metadata": {
        "id": "rZCzdZZ9FMhm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c9wipRZt7feC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NewsEncoder(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings, \n",
        "                 num_input_channels, num_output_channels, hidden_dim, \n",
        "                 num_layers, bidirectional, word_padding_idx=0, \n",
        "                 char_padding_idx=0):\n",
        "        super(NewsEncoder, self).__init__()\n",
        "        \n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        \n",
        "        # Embeddings\n",
        "        self.word_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                            num_embeddings=num_word_embeddings,\n",
        "                                            padding_idx=word_padding_idx)\n",
        "        self.char_embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                           num_embeddings=num_char_embeddings,\n",
        "                                           padding_idx=char_padding_idx)\n",
        "        \n",
        "        # Conv weights\n",
        "        self.conv = nn.ModuleList(\n",
        "            [nn.Conv2d(num_input_channels, num_output_channels, (1, f)) \\\n",
        "             for f in [5]])\n",
        "        \n",
        "        # GRU weights\n",
        "        self.gru = nn.GRU(input_size=embedding_dim*2, hidden_size=hidden_dim, \n",
        "                          num_layers=num_layers, batch_first=True, \n",
        "                          bidirectional=bidirectional)\n",
        "        \n",
        "    def initialize_hidden_state(self, batch_size, hidden_dim):\n",
        "        num_directions = 1\n",
        "        if self.bidirectional:\n",
        "            num_directions = 2\n",
        "        return torch.zeros(self.num_layers * num_directions, batch_size, \n",
        "                           hidden_dim)\n",
        "\n",
        "    def forward(self, x_in_word, x_in_char, x_lengths):\n",
        "        \"\"\"\n",
        "        x_in_word: word level representation (N, seq_size)\n",
        "        x_In_char: char level representation (N, seq_size, word_size)\n",
        "        \"\"\"\n",
        "        \n",
        "        # Word level embeddings\n",
        "        z_word = self.word_embeddings(x_in_word)\n",
        "        \n",
        "        # Char-level embeddings\n",
        "        # x: (N, seq_len, word_len)\n",
        "        input_shape = x_in_char.size()\n",
        "        word_len = x_in_char.size(2)\n",
        "        x_in_char = x_in_char.view(-1, word_len) # (N*seq_len, word_len)\n",
        "        \n",
        "        # Embedding\n",
        "        x_char_embedded = self.char_embeddings(x_in_char) # (N*seq_len, word_len, embedding_dim)\n",
        "        x_char_embedded = x_char_embedded.view(*input_shape, -1) # (N, seq_len, word_len, embedding_dim)\n",
        "        \n",
        "        # Sum character level embeddings for each word\n",
        "        x_char_embedded = x_char_embedded.sum(2) # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # insert channel dim\n",
        "        x_char_embedded = x_char_embedded.unsqueeze(1) # (N, 1, seq_len, embedding_dim)\n",
        "        \n",
        "        # CNN\n",
        "        ### (N, embedding_dim, seq_len, (embedding_dim - filter_size + 1))\n",
        "        z_char = [F.relu(conv(x_char_embedded)) for conv in self.conv]\n",
        "        ### (N, seq_len, (embedding_dim - filter_size + 1), embedding_dim)\n",
        "        z_char = [zz.view((zz.size(0), zz.size(2), zz.size(3), zz.size(1))) \\\n",
        "             for zz in z_char]\n",
        "        \n",
        "        # Pooling\n",
        "        z_char = [torch.sum(zz, 2) for zz in z_char] # (N, seq_len, embedding_dim)\n",
        "        \n",
        "        # Concat to get char-level embeddings\n",
        "        z_char = torch.cat(z_char, 2) # join conv outputs\n",
        "        \n",
        "        # Concat word-level and char-level embeddings\n",
        "        z = torch.cat([z_word, z_char], 2)\n",
        "            \n",
        "        # Feed into RNN\n",
        "        initial_h = self.initialize_hidden_state(\n",
        "            batch_size=z.size(0), hidden_dim=self.gru.hidden_size)\n",
        "        out, h_n = self.gru(z, initial_h)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zeEcdA287gz4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NewsDecoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim, dropout_p):\n",
        "        super(NewsDecoder, self).__init__()\n",
        "        \n",
        "        # Attention FC layer\n",
        "        self.fc_attn = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "        \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, encoder_outputs, apply_softmax=False):\n",
        "        \n",
        "        # Attention\n",
        "        z = torch.tanh(self.fc_attn(encoder_outputs))\n",
        "        z = z.transpose(2,1) # [B*H*T]\n",
        "        v = self.v.repeat(encoder_outputs.size(0),1).unsqueeze(1) #[B*1*H]\n",
        "        z = torch.bmm(v,z).squeeze(1) # [B*T]\n",
        "        what2 = z\n",
        "        attn_scores = F.softmax(z, dim=1)\n",
        "        what3 = attn_scores\n",
        "        context = torch.matmul(encoder_outputs.transpose(-2, -1), \n",
        "                               attn_scores.unsqueeze(dim=2)).squeeze()\n",
        "        if len(context.size()) == 1:\n",
        "            context = context.unsqueeze(0)\n",
        "        \n",
        "        # FC layers\n",
        "        z = self.dropout(context)\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        y_pred = self.fc2(z)\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_pred = F.softmax(y_pred, dim=1)\n",
        "        return attn_scores, y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yVDftS-G7gwy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NewsModel(nn.Module):\n",
        "    def __init__(self, embedding_dim, num_word_embeddings, num_char_embeddings, \n",
        "                 num_input_channels, num_output_channels, hidden_dim, \n",
        "                 output_dim, num_layers, bidirectional, dropout_p, \n",
        "                 word_padding_idx, char_padding_idx):\n",
        "        super(NewsModel, self).__init__()\n",
        "        self.encoder = NewsEncoder(embedding_dim, num_word_embeddings, \n",
        "                                   num_char_embeddings, num_input_channels, \n",
        "                                   num_output_channels, hidden_dim, num_layers, \n",
        "                                   bidirectional, word_padding_idx, \n",
        "                                   char_padding_idx)\n",
        "        self.decoder = NewsDecoder(hidden_dim, output_dim, dropout_p)\n",
        "        \n",
        "    def forward(self, x_in_word, x_in_char, x_lengths, apply_softmax=False):\n",
        "        encoder_outputs = self.encoder(x_in_word, x_in_char, x_lengths)\n",
        "        y_pred = self.decoder(encoder_outputs, apply_softmax)\n",
        "        return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jHPYCPd7Fl3M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "D3seBMA7FlcC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ouzGGzdChJcF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ]
    },
    {
      "metadata": {
        "id": "HnRKWLekFlnM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, dataset, model, model_state_file, save_dir, device, shuffle, \n",
        "               num_epochs, batch_size, learning_rate, early_stopping_criteria):\n",
        "        self.dataset = dataset\n",
        "        self.class_weights = dataset.class_weights.to(device)\n",
        "        self.model = model.to(device)\n",
        "        self.save_dir = save_dir\n",
        "        self.device = device\n",
        "        self.shuffle = shuffle\n",
        "        self.num_epochs = num_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.loss_func = nn.CrossEntropyLoss(self.class_weights)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer=self.optimizer, mode='min', factor=0.5, patience=1)\n",
        "        self.train_state = {\n",
        "            'stop_early': False, \n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'early_stopping_criteria': early_stopping_criteria,\n",
        "            'learning_rate': learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': model_state_file}\n",
        "    \n",
        "    def update_train_state(self):\n",
        "\n",
        "        # Verbose\n",
        "        print (\"[EPOCH]: {0} | [LR]: {1} | [TRAIN LOSS]: {2:.2f} | [TRAIN ACC]: {3:.1f}% | [VAL LOSS]: {4:.2f} | [VAL ACC]: {5:.1f}%\".format(\n",
        "          self.train_state['epoch_index'], self.train_state['learning_rate'], \n",
        "            self.train_state['train_loss'][-1], self.train_state['train_acc'][-1], \n",
        "            self.train_state['val_loss'][-1], self.train_state['val_acc'][-1]))\n",
        "\n",
        "        # Save one model at least\n",
        "        if self.train_state['epoch_index'] == 0:\n",
        "            torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "            self.train_state['stop_early'] = False\n",
        "\n",
        "        # Save model if performance improved\n",
        "        elif self.train_state['epoch_index'] >= 1:\n",
        "            loss_tm1, loss_t = self.train_state['val_loss'][-2:]\n",
        "\n",
        "            # If loss worsened\n",
        "            if loss_t >= self.train_state['early_stopping_best_val']:\n",
        "                # Update step\n",
        "                self.train_state['early_stopping_step'] += 1\n",
        "\n",
        "            # Loss decreased\n",
        "            else:\n",
        "                # Save the best model\n",
        "                if loss_t < self.train_state['early_stopping_best_val']:\n",
        "                    torch.save(self.model.state_dict(), self.train_state['model_filename'])\n",
        "\n",
        "                # Reset early stopping step\n",
        "                self.train_state['early_stopping_step'] = 0\n",
        "\n",
        "            # Stop early ?\n",
        "            self.train_state['stop_early'] = self.train_state['early_stopping_step'] \\\n",
        "              >= self.train_state['early_stopping_criteria']\n",
        "        return self.train_state\n",
        "  \n",
        "    def compute_accuracy(self, y_pred, y_target):\n",
        "        _, y_pred_indices = y_pred.max(dim=1)\n",
        "        n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "        return n_correct / len(y_pred_indices) * 100\n",
        "  \n",
        "    def run_train_loop(self):\n",
        "        for epoch_index in range(self.num_epochs):\n",
        "            self.train_state['epoch_index'] = epoch_index\n",
        "      \n",
        "            # Iterate over train dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "            self.dataset.set_split('train')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, \n",
        "                device=self.device)\n",
        "            running_loss = 0.0\n",
        "            running_acc = 0.0\n",
        "            self.model.train()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "                # the training routine is these 5 steps:\n",
        "\n",
        "                # --------------------------------------\n",
        "                # step 1. zero the gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                \n",
        "                # step 2. compute the output\n",
        "                _, y_pred = self.model(x_in_word=batch_dict['title_word_vector'], \n",
        "                                    x_in_char=batch_dict['title_char_vector'],\n",
        "                                    x_lengths=batch_dict['title_length'])\n",
        "                \n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "                loss_t = loss.item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # step 4. use loss to produce gradients\n",
        "                loss.backward()\n",
        "\n",
        "                # step 5. use optimizer to take gradient step\n",
        "                self.optimizer.step()\n",
        "                # -----------------------------------------\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['train_loss'].append(running_loss)\n",
        "            self.train_state['train_acc'].append(running_acc)\n",
        "\n",
        "            # Iterate over val dataset\n",
        "\n",
        "            # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "            self.dataset.set_split('val')\n",
        "            batch_generator = self.dataset.generate_batches(\n",
        "                batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "            running_loss = 0.\n",
        "            running_acc = 0.\n",
        "            self.model.eval()\n",
        "\n",
        "            for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "                # compute the output\n",
        "                _, y_pred = self.model(x_in_word=batch_dict['title_word_vector'], \n",
        "                                    x_in_char=batch_dict['title_char_vector'],\n",
        "                                    x_lengths=batch_dict['title_length'])\n",
        "\n",
        "                # step 3. compute the loss\n",
        "                loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "                loss_t = loss.to(\"cpu\").item()\n",
        "                running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "                # compute the accuracy\n",
        "                acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "                running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "            self.train_state['val_loss'].append(running_loss)\n",
        "            self.train_state['val_acc'].append(running_acc)\n",
        "\n",
        "            self.train_state = self.update_train_state()\n",
        "            self.scheduler.step(self.train_state['val_loss'][-1])\n",
        "            if self.train_state['stop_early']:\n",
        "                break\n",
        "          \n",
        "    def run_test_loop(self):\n",
        "        self.dataset.set_split('test')\n",
        "        batch_generator = self.dataset.generate_batches(\n",
        "            batch_size=self.batch_size, shuffle=self.shuffle, device=self.device)\n",
        "        running_loss = 0.0\n",
        "        running_acc = 0.0\n",
        "        self.model.eval()\n",
        "\n",
        "        for batch_index, batch_dict in enumerate(batch_generator):\n",
        "            # compute the output\n",
        "            _, y_pred = self.model(x_in_word=batch_dict['title_word_vector'], \n",
        "                                x_in_char=batch_dict['title_char_vector'],\n",
        "                                x_lengths=batch_dict['title_length'])\n",
        "\n",
        "            # compute the loss\n",
        "            loss = self.loss_func(y_pred, batch_dict['category'])\n",
        "            loss_t = loss.item()\n",
        "            running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "            # compute the accuracy\n",
        "            acc_t = self.compute_accuracy(y_pred, batch_dict['category'])\n",
        "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "        self.train_state['test_loss'] = running_loss\n",
        "        self.train_state['test_acc'] = running_acc\n",
        "    \n",
        "    def plot_performance(self):\n",
        "        # Figure size\n",
        "        plt.figure(figsize=(15,5))\n",
        "\n",
        "        # Plot Loss\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.title(\"Loss\")\n",
        "        plt.plot(trainer.train_state[\"train_loss\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_loss\"], label=\"val\")\n",
        "        plt.legend(loc='upper right')\n",
        "\n",
        "        # Plot Accuracy\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.title(\"Accuracy\")\n",
        "        plt.plot(trainer.train_state[\"train_acc\"], label=\"train\")\n",
        "        plt.plot(trainer.train_state[\"val_acc\"], label=\"val\")\n",
        "        plt.legend(loc='lower right')\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(self.save_dir, \"performance.png\"))\n",
        "\n",
        "        # Show plots\n",
        "        plt.show()\n",
        "    \n",
        "    def save_train_state(self):\n",
        "        with open(os.path.join(self.save_dir, \"train_state.json\"), \"w\") as fp:\n",
        "            json.dump(self.train_state, fp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SDxeN7RzhNCP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Trainer ends"
      ]
    },
    {
      "metadata": {
        "id": "ICkiOaGtFlk-",
        "colab_type": "code",
        "outputId": "056f0545-95df-490c-87ee-6190ad3f02b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# Initialization\n",
        "if args.reload_from_files:\n",
        "    print (\"Reloading!\")\n",
        "    dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
        "        args.split_data_file,args.vectorizer_file)\n",
        "else:\n",
        "    print (\"Creating from scratch!\")\n",
        "    dataset = NewsDataset.load_dataset_and_make_vectorizer(args.split_data_file)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = NewsModel(embedding_dim=args.embedding_dim, \n",
        "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
        "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
        "                  num_input_channels=1, \n",
        "                  num_output_channels=args.num_filters,\n",
        "                  hidden_dim=args.hidden_dim,\n",
        "                  output_dim=len(vectorizer.category_vocab),\n",
        "                  num_layers=args.num_layers,\n",
        "                  bidirectional=args.bidirectional,\n",
        "                  dropout_p=args.dropout_p, \n",
        "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
        "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating from scratch!\n",
            "<bound method Module.named_modules of NewsModel(\n",
            "  (encoder): NewsEncoder(\n",
            "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
            "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
            "    (conv): ModuleList(\n",
            "      (0): Conv2d(1, 100, kernel_size=(1, 5), stride=(1, 1))\n",
            "    )\n",
            "    (gru): GRU(100, 100, batch_first=True)\n",
            "  )\n",
            "  (decoder): NewsDecoder(\n",
            "    (fc_attn): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (dropout): Dropout(p=0.1)\n",
            "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tuaRZ4DiFlh1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train\n",
        "trainer = Trainer(dataset=dataset, model=model, \n",
        "                  model_state_file=args.model_state_file, \n",
        "                  save_dir=args.save_dir, device=args.device,\n",
        "                  shuffle=args.shuffle, num_epochs=args.num_epochs, \n",
        "                  batch_size=args.batch_size, learning_rate=args.learning_rate, \n",
        "                  early_stopping_criteria=args.early_stopping_criteria)\n",
        "trainer.run_train_loop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mzRJIz88Flfe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot performance\n",
        "trainer.plot_performance()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4EmFhiX-FMaV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test performance\n",
        "trainer.run_test_loop()\n",
        "print(\"Test loss: {0:.2f}\".format(trainer.train_state['test_loss']))\n",
        "print(\"Test Accuracy: {0:.1f}%\".format(trainer.train_state['test_acc']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zVU1zakYFMVF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save all results\n",
        "trainer.save_train_state()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qLoKfjSpFw7t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ]
    },
    {
      "metadata": {
        "id": "ANrPcS7Hp_CP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Inference(object):\n",
        "    def __init__(self, model, vectorizer):\n",
        "        self.model = model\n",
        "        self.vectorizer = vectorizer\n",
        "  \n",
        "    def predict_category(self, title):\n",
        "        # Vectorize\n",
        "        word_vector, char_vector, title_length = self.vectorizer.vectorize(title)\n",
        "        title_word_vector = torch.tensor(word_vector).unsqueeze(0)\n",
        "        title_char_vector = torch.tensor(char_vector).unsqueeze(0)\n",
        "        title_length = torch.tensor([title_length]).long()        \n",
        "        \n",
        "        # Forward pass\n",
        "        self.model.eval()\n",
        "        attn_scores, y_pred = self.model(x_in_word=title_word_vector, \n",
        "                            x_in_char=title_char_vector,\n",
        "                            x_lengths=title_length, \n",
        "                            apply_softmax=True)\n",
        "\n",
        "        # Top category\n",
        "        y_prob, indices = y_pred.max(dim=1)\n",
        "        index = indices.item()\n",
        "\n",
        "        # Predicted category\n",
        "        category = vectorizer.category_vocab.lookup_index(index)\n",
        "        probability = y_prob.item()\n",
        "        return {'category': category, 'probability': probability, \n",
        "                'attn_scores': attn_scores}\n",
        "    \n",
        "    def predict_top_k(self, title, k):\n",
        "        # Vectorize\n",
        "        word_vector, char_vector, title_length = self.vectorizer.vectorize(title)\n",
        "        title_word_vector = torch.tensor(word_vector).unsqueeze(0)\n",
        "        title_char_vector = torch.tensor(char_vector).unsqueeze(0)\n",
        "        title_length = torch.tensor([title_length]).long()\n",
        "        \n",
        "         # Forward pass\n",
        "        self.model.eval()\n",
        "        _, y_pred = self.model(x_in_word=title_word_vector, \n",
        "                            x_in_char=title_char_vector,\n",
        "                            x_lengths=title_length, \n",
        "                            apply_softmax=True)\n",
        "        \n",
        "        # Top k categories\n",
        "        y_prob, indices = torch.topk(y_pred, k=k)\n",
        "        probabilities = y_prob.detach().numpy()[0]\n",
        "        indices = indices.detach().numpy()[0]\n",
        "\n",
        "        # Results\n",
        "        results = []\n",
        "        for probability, index in zip(probabilities, indices):\n",
        "            category = self.vectorizer.category_vocab.lookup_index(index)\n",
        "            results.append({'category': category, 'probability': probability})\n",
        "\n",
        "        return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W6wr68o2p_Eh",
        "colab_type": "code",
        "outputId": "338c9546-63b3-4946-aa2f-898a1af1f1f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "print (\"Reloading!\")\n",
        "dataset = NewsDataset.load_dataset_and_load_vectorizer(\n",
        "    args.split_data_file, args.vectorizer_file)\n",
        "vectorizer = dataset.vectorizer\n",
        "model = NewsModel(embedding_dim=args.embedding_dim, \n",
        "                  num_word_embeddings=len(vectorizer.title_word_vocab), \n",
        "                  num_char_embeddings=len(vectorizer.title_char_vocab),\n",
        "                  num_input_channels=1, \n",
        "                  num_output_channels=args.num_filters,\n",
        "                  hidden_dim=args.hidden_dim,\n",
        "                  output_dim=len(vectorizer.category_vocab),\n",
        "                  num_layers=args.num_layers,\n",
        "                  bidirectional=args.bidirectional,\n",
        "                  dropout_p=args.dropout_p, \n",
        "                  word_padding_idx=vectorizer.title_word_vocab.mask_index,\n",
        "                  char_padding_idx=vectorizer.title_char_vocab.mask_index)\n",
        "model.load_state_dict(torch.load(args.model_state_file))\n",
        "model = model.to(args.device)\n",
        "print (model.named_modules)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reloading!\n",
            "<bound method Module.named_modules of NewsModel(\n",
            "  (encoder): NewsEncoder(\n",
            "    (word_embeddings): Embedding(3406, 100, padding_idx=0)\n",
            "    (char_embeddings): Embedding(35, 100, padding_idx=0)\n",
            "    (conv): ModuleList(\n",
            "      (0): Conv2d(1, 100, kernel_size=(1, 5), stride=(1, 1))\n",
            "    )\n",
            "    (gru): GRU(100, 100, batch_first=True)\n",
            "  )\n",
            "  (decoder): NewsDecoder(\n",
            "    (fc_attn): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (dropout): Dropout(p=0.1)\n",
            "    (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
            "    (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            "  )\n",
            ")>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JPKgHxsfN954",
        "colab_type": "code",
        "outputId": "793531cd-218f-4353-a080-2c90dd2fdf93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Inference\n",
        "inference = Inference(model=model, vectorizer=vectorizer)\n",
        "title = input(\"Enter a title to classify: \")\n",
        "prediction = inference.predict_category(preprocess_text(title))\n",
        "print(\"{}  {} (p={:0.2f})\".format(title, prediction['category'], \n",
        "                                    prediction['probability']))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter a title to classify: Roger Federer won the tennis tournament.\n",
            "Roger Federer won the tennis tournament.  Sports (p=0.87)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JRdz4wzuQR4N",
        "colab_type": "code",
        "outputId": "671ef540-6cbf-46be-87b1-d44859ee89c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Top-k inference\n",
        "top_k = inference.predict_top_k(preprocess_text(title), k=len(vectorizer.category_vocab))\n",
        "for result in top_k:\n",
        "    print (\"{}  {} (p={:0.2f})\".format(title, result['category'], \n",
        "                                         result['probability']))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Roger Federer won the tennis tournament.  Sports (p=0.87)\n",
            "Roger Federer won the tennis tournament.  World (p=0.09)\n",
            "Roger Federer won the tennis tournament.  Sci/Tech (p=0.02)\n",
            "Roger Federer won the tennis tournament.  Business (p=0.02)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R3jrZ6ZkxN4r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Interpretability"
      ]
    },
    {
      "metadata": {
        "id": "qrAieHoHxOt2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can inspect the probability vector that is generated at each time step to visualize the importance of each of the previous hidden states towards a particular time step's prediction. "
      ]
    },
    {
      "metadata": {
        "id": "k6uZY4J8vYgw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S6eL_X23xOEM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 330
        },
        "outputId": "855961f8-9c69-4123-fbd5-8c4b8aa340b5"
      },
      "cell_type": "code",
      "source": [
        "attn_matrix = prediction['attn_scores'].detach().numpy()\n",
        "ax = sns.heatmap(attn_matrix, linewidths=2, square=True)\n",
        "tokens = [\"<BEGIN>\"]+preprocess_text(title).split(\" \")+[\"<END>\"]\n",
        "ax.set_xticklabels(tokens, rotation=45)\n",
        "ax.set_xlabel(\"Token\")\n",
        "ax.set_ylabel(\"Importance\\n\")\n",
        "plt.show()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd4AAAE5CAYAAAA+30H5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f06728c4400>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1YHneO3SStOp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TODO"
      ]
    },
    {
      "metadata": {
        "id": "gGHaKTe1SuEk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- bleu score\n",
        "- ngram-overlap\n",
        "- perplexity\n",
        "- beamsearch\n",
        "- hierarchical softmax\n",
        "- hierarchical attention\n",
        "- Transformer networks\n",
        "- Increase performance with char and word level embeddings (not so great right now)"
      ]
    }
  ]
}