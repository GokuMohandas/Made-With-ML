{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgZTKjbZ6kfz"
      },
      "source": [
        "<div align=\"center\">\n",
        "<h1><img width=\"30\" src=\"https://madewithml.com/static/images/rounded_logo.png\">&nbsp;<a href=\"https://madewithml.com/\">Made With ML</a></h1>\n",
        "Applied ML Â· MLOps Â· Production\n",
        "<br>\n",
        "Join 30K+ developers in learning how to responsibly <a href=\"https://madewithml.com/about/\">deliver value</a> with ML.\n",
        "    <br>\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "    <a target=\"_blank\" href=\"https://madewithml.com\"><img src=\"https://img.shields.io/badge/Subscribe-30K-brightgreen\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://github.com/GokuMohandas/Made-With-ML\"><img src=\"https://img.shields.io/github/stars/GokuMohandas/Made-With-ML.svg?style=social&label=Star\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://www.linkedin.com/in/goku\"><img src=\"https://img.shields.io/badge/style--5eba00.svg?label=LinkedIn&logo=linkedin&style=social\"></a>&nbsp;\n",
        "    <a target=\"_blank\" href=\"https://twitter.com/GokuMohandas\"><img src=\"https://img.shields.io/twitter/follow/GokuMohandas.svg?label=Follow&style=social\"></a>\n",
        "    <br>\n",
        "    ðŸ”¥&nbsp; Among the <a href=\"https://github.com/topics/deep-learning\" target=\"_blank\">top ML</a> repositories on GitHub\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTdCMVl9YAXw"
      },
      "source": [
        "# Recurrent Neural Networks (RNN)\n",
        "\n",
        "In this lesson we will learn how to process sequential data (sentences, time-series, etc.) with recurrent neural networks (RNNs)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuabAj4PYj57"
      },
      "source": [
        "<div align=\"left\">\n",
        "<a target=\"_blank\" href=\"https://madewithml.com/courses/foundations/recurrent-neural-networks/\"><img src=\"https://img.shields.io/badge/ðŸ“– Read-blog post-9cf\"></a>&nbsp;\n",
        "<a href=\"https://github.com/GokuMohandas/Made-With-ML/blob/main/notebooks/13_Recurrent_Neural_Networks.ipynb\" role=\"button\"><img src=\"https://img.shields.io/static/v1?label=&amp;message=View%20On%20GitHub&amp;color=586069&amp;logo=github&amp;labelColor=2f363d\"></a>&nbsp;\n",
        "<a href=\"https://colab.research.google.com/github/GokuMohandas/Made-With-ML/blob/main/notebooks/13_Recurrent_Neural_Networks.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXuhm14WaCxy"
      },
      "source": [
        "So far we've processed inputs as whole (ex. applying filters across the entire input to extract features) but we can also process our inputs sequentially. For example we can think of each token in our text as an event in time (timestep). We can process each timestep, one at a time, and predict the class after the last timestep (token) has been processed. This is very powerful because the model now has a meaningful way to account for the sequential order of tokens in our sequence and predict accordingly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5JJY0tl8StF"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34Yf1pdURtLn"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://madewithml.com/static/images/foundations/rnn/vanilla.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "RNN forward pass for a single time step $X_t$:\n",
        "\n",
        "$h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h)$\n",
        "\n",
        "*where*:\n",
        "* $W_{hh}$ = hidden units weights| $\\in \\mathbb{R}^{HXH}$ ($H$ is the hidden dim)\n",
        "* $h_{t-1}$ = previous timestep's hidden state $\\in \\mathbb{R}^{NXH}$\n",
        "* $W_{xh}$ = input weights| $\\in \\mathbb{R}^{EXH}$\n",
        "* $X_t$ = input at time step t | $\\in \\mathbb{R}^{NXE}$ ($N$ is the batch size, $E$ is the embedding dim)\n",
        "* $b_h$ = hidden units bias $\\in \\mathbb{R}^{HX1}$\n",
        "* $h_t$ = output from RNN for timestep $t$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqxyljU18hvt"
      },
      "source": [
        "* **Objective:**  Process sequential data by accounting for the currend input and also what has been learned from previous inputs.\n",
        "* **Advantages:** \n",
        "    * Account for order and previous inputs in a meaningful way.\n",
        "    * Conditioned generation for generating sequences.\n",
        "* **Disadvantages:** \n",
        "    * Each time step's prediction depends on the previous prediction so it's difficult to parallelize RNN operations. \n",
        "    * Processing long sequences can yield memory and computation issues.\n",
        "    * Interpretability is difficult but there are few [techniques](https://arxiv.org/abs/1506.02078) that use the activations from RNNs to see what parts of the inputs are processed. \n",
        "* **Miscellaneous:** \n",
        "    * Architectural tweaks to make RNNs faster and interpretable is an ongoing area of research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V88LZeTqDePU"
      },
      "source": [
        "# Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DlRv80v5DfdP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NoNhL0wTDgLW",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "SEED = 1234"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gwVLi0UPDgOd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=1234):\n",
        "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed) # multi-GPU# Set seeds for reproducibility\n",
        "set_seeds(seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HK5Onfz9DiPO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Set seeds for reproducibility\n",
        "set_seeds(seed=SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4T1rZDhsDiRt",
        "outputId": "0c1fbb20-61e3-4d14-91fe-e4da6b4f0335",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "cuda = True\n",
        "device = torch.device(\"cuda\" if (\n",
        "    torch.cuda.is_available() and cuda) else \"cpu\")\n",
        "torch.set_default_tensor_type(\"torch.FloatTensor\")\n",
        "if device.type == \"cuda\":\n",
        "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c69z9wpJ56nE"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V_nEp5G58M0"
      },
      "source": [
        "We will download the [AG News dataset](http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html), which consists of 120K text samples from 4 unique classes (`Business`, `Sci/Tech`, `Sports`, `World`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y3qKSoEe57na",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import urllib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "BqaHCtToXWAb",
        "outputId": "c0ff2a72-f090-4465-e089-e968babdc895",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Sharon Accepts Plan to Reduce Gaza Army Operat...</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Internet Key Battleground in Wildlife Crime Fight</td>\n",
              "      <td>Sci/Tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>July Durable Good Orders Rise 1.7 Percent</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Growing Signs of a Slowing on Wall Street</td>\n",
              "      <td>Business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The New Faces of Reality TV</td>\n",
              "      <td>World</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               title  category\n",
              "0  Sharon Accepts Plan to Reduce Gaza Army Operat...     World\n",
              "1  Internet Key Battleground in Wildlife Crime Fight  Sci/Tech\n",
              "2          July Durable Good Orders Rise 1.7 Percent  Business\n",
              "3          Growing Signs of a Slowing on Wall Street  Business\n",
              "4                        The New Faces of Reality TV     World"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/GokuMohandas/Made-With-ML/main/datasets/news.csv\"\n",
        "df = pd.read_csv(url, header=0) # load\n",
        "df = df.sample(frac=1).reset_index(drop=True) # shuffle\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HR_lLiVBDxW1"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u00AZ0O8DxaY"
      },
      "source": [
        "We're going to clean up our input data first by doing operations such as lower text, removing stop (filler) words, filters using regular expressions, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Jq6a11YsDvEU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5IpVaKqDvG1",
        "outputId": "b0e0fe6d-ac43-4e5b-da15-5a3dfad2b80b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we']\n"
          ]
        }
      ],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "STOPWORDS = stopwords.words(\"english\")\n",
        "print (STOPWORDS[:5])\n",
        "porter = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1n4w3KLQDvJk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def preprocess(text, stopwords=STOPWORDS):\n",
        "    \"\"\"Conditional preprocessing on our text unique to our task.\"\"\"\n",
        "    # Lower\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove stopwords\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "\n",
        "    # Remove words in paranthesis\n",
        "    text = re.sub(r'\\([^)]*\\)', '', text)\n",
        "\n",
        "    # Spacing and filters\n",
        "    text = re.sub(r\"([-;;.,!?<=>])\", r\" \\1 \", text)\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text) # remove non alphanumeric chars\n",
        "    text = re.sub(' +', ' ', text)  # remove multiple spaces\n",
        "    text = text.strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "U9JmpiCmDvMz",
        "outputId": "c7e9ff39-c35b-4ded-838c-ffb71de93e65",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'great week nyse'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Sample\n",
        "text = \"Great week for the NYSE!\"\n",
        "preprocess(text=text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RZ45-N9DvQD",
        "outputId": "b731b9ac-cc6c-4602-ea47-c80e268bb155",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sharon Accepts Plan to Reduce Gaza Army Operation, Haaretz Says\n",
            "\n",
            "sharon accepts plan reduce gaza army operation haaretz says\n"
          ]
        }
      ],
      "source": [
        "# Apply to dataframe\n",
        "preprocessed_df = df.copy()\n",
        "preprocessed_df.title = preprocessed_df.title.apply(preprocess)\n",
        "print (f\"{df.title.values[0]}\\n\\n{preprocessed_df.title.values[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxo6RKCQ71dl"
      },
      "source": [
        "## Split data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eS6kCcfY6IHE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4qkWnO3uYPnk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "TRAIN_SIZE = 0.7\n",
        "VAL_SIZE = 0.15\n",
        "TEST_SIZE = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0gbh2TLBXVDu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def train_val_test_split(X, y, train_size):\n",
        "    \"\"\"Split dataset into data splits.\"\"\"\n",
        "    X_train, X_, y_train, y_ = train_test_split(X, y, train_size=TRAIN_SIZE, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_, y_, train_size=0.5, stratify=y_)\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kqiQd2j_76gP",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "X = preprocessed_df[\"title\"].values\n",
        "y = preprocessed_df[\"category\"].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwdPM136EBMA",
        "outputId": "2dbb797f-eed8-4e57-c8bb-3e75cd9ec3eb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train: (84000,), y_train: (84000,)\n",
            "X_val: (18000,), y_val: (18000,)\n",
            "X_test: (18000,), y_test: (18000,)\n",
            "Sample point: china battles north korea nuclear talks â†’ World\n"
          ]
        }
      ],
      "source": [
        "# Create data splits\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = train_val_test_split(\n",
        "    X=X, y=y, train_size=TRAIN_SIZE)\n",
        "print (f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print (f\"X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "print (f\"X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
        "print (f\"Sample point: {X_train[0]} â†’ {y_train[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUAn5IOjEF8P"
      },
      "source": [
        "## LabelEncoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNTjndI0EKzm"
      },
      "source": [
        "Next we'll define a `LabelEncoder` to encode our text labels into unique indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OgLNA5yTELB1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GeoPR77QELJT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class LabelEncoder(object):\n",
        "    \"\"\"Label encoder for tag labels.\"\"\"\n",
        "    def __init__(self, class_to_index={}):\n",
        "        self.class_to_index = class_to_index or {}  # mutable defaults ;)\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "        self.classes = list(self.class_to_index.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.class_to_index)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<LabelEncoder(num_classes={len(self)})>\"\n",
        "\n",
        "    def fit(self, y):\n",
        "        classes = np.unique(y)\n",
        "        for i, class_ in enumerate(classes):\n",
        "            self.class_to_index[class_] = i\n",
        "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
        "        self.classes = list(self.class_to_index.keys())\n",
        "        return self\n",
        "\n",
        "    def encode(self, y):\n",
        "        encoded = np.zeros((len(y)), dtype=int)\n",
        "        for i, item in enumerate(y):\n",
        "            encoded[i] = self.class_to_index[item]\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, y):\n",
        "        classes = []\n",
        "        for i, item in enumerate(y):\n",
        "            classes.append(self.index_to_class[item])\n",
        "        return classes\n",
        "\n",
        "    def save(self, fp):\n",
        "        with open(fp, \"w\") as fp:\n",
        "            contents = {'class_to_index': self.class_to_index}\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, fp):\n",
        "        with open(fp, \"r\") as fp:\n",
        "            kwargs = json.load(fp=fp)\n",
        "        return cls(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MtTxMB5ELL4",
        "outputId": "77036c78-184a-4cb8-ca4c-814ba006bde0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Business': 0, 'Sci/Tech': 1, 'Sports': 2, 'World': 3}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encode\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "NUM_CLASSES = len(label_encoder)\n",
        "label_encoder.class_to_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IzPFnoDELSr",
        "outputId": "b9ced4b7-deaa-4cd6-a955-dceb11f77c1d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "y_train[0]: World\n",
            "y_train[0]: 3\n"
          ]
        }
      ],
      "source": [
        "# Convert labels to tokens\n",
        "print (f\"y_train[0]: {y_train[0]}\")\n",
        "y_train = label_encoder.encode(y_train)\n",
        "y_val = label_encoder.encode(y_val)\n",
        "y_test = label_encoder.encode(y_test)\n",
        "print (f\"y_train[0]: {y_train[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjESU2TOEQz_",
        "outputId": "ced4b758-a4ea-44be-bb40-eca4bcc313ae",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "counts: [21000 21000 21000 21000]\n",
            "weights: {0: 4.761904761904762e-05, 1: 4.761904761904762e-05, 2: 4.761904761904762e-05, 3: 4.761904761904762e-05}\n"
          ]
        }
      ],
      "source": [
        "# Class weights\n",
        "counts = np.bincount(y_train)\n",
        "class_weights = {i: 1.0/count for i, count in enumerate(counts)}\n",
        "print (f\"counts: {counts}\\nweights: {class_weights}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIfmW7vJ8Jx1"
      },
      "source": [
        "## Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYREXERdETdQ"
      },
      "source": [
        "We'll define a `Tokenizer` to convert our text input data into token indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0V6v1yeVGxNg",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "from more_itertools import take"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ChhTN8WDY4Z5",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self, char_level, num_tokens=None, \n",
        "                 pad_token='<PAD>', oov_token='<UNK>',\n",
        "                 token_to_index=None):\n",
        "        self.char_level = char_level\n",
        "        self.separator = '' if self.char_level else ' '\n",
        "        if num_tokens: num_tokens -= 2 # pad + unk tokens\n",
        "        self.num_tokens = num_tokens\n",
        "        self.pad_token = pad_token\n",
        "        self.oov_token = oov_token\n",
        "        if not token_to_index:\n",
        "            token_to_index = {pad_token: 0, oov_token: 1}\n",
        "        self.token_to_index = token_to_index\n",
        "        self.index_to_token = {v: k for k, v in self.token_to_index.items()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_to_index)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<Tokenizer(num_tokens={len(self)})>\"\n",
        "\n",
        "    def fit_on_texts(self, texts):\n",
        "        if not self.char_level:\n",
        "            texts = [text.split(\" \") for text in texts]\n",
        "        all_tokens = [token for text in texts for token in text]\n",
        "        counts = Counter(all_tokens).most_common(self.num_tokens)\n",
        "        self.min_token_freq = counts[-1][1]\n",
        "        for token, count in counts:\n",
        "            index = len(self)\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "        return self\n",
        "\n",
        "    def texts_to_sequences(self, texts):\n",
        "        sequences = []\n",
        "        for text in texts:\n",
        "            if not self.char_level:\n",
        "                text = text.split(' ')\n",
        "            sequence = []\n",
        "            for token in text:\n",
        "                sequence.append(self.token_to_index.get(\n",
        "                    token, self.token_to_index[self.oov_token]))\n",
        "            sequences.append(np.asarray(sequence))\n",
        "        return sequences\n",
        "\n",
        "    def sequences_to_texts(self, sequences):\n",
        "        texts = []\n",
        "        for sequence in sequences:\n",
        "            text = []\n",
        "            for index in sequence:\n",
        "                text.append(self.index_to_token.get(index, self.oov_token))\n",
        "            texts.append(self.separator.join([token for token in text]))\n",
        "        return texts\n",
        "\n",
        "    def save(self, fp):\n",
        "        with open(fp, \"w\") as fp:\n",
        "            contents = {\n",
        "                \"char_level\": self.char_level,\n",
        "                \"oov_token\": self.oov_token,\n",
        "                \"token_to_index\": self.token_to_index\n",
        "            }\n",
        "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, fp):\n",
        "        with open(fp, \"r\") as fp:\n",
        "            kwargs = json.load(fp=fp)\n",
        "        return cls(**kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylpHSbcZYsXa",
        "outputId": "bfaf2869-8096-4a7b-97e9-19b897c8ed67",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<Tokenizer(num_tokens=5000)>\n"
          ]
        }
      ],
      "source": [
        "# Tokenize\n",
        "tokenizer = Tokenizer(char_level=False, num_tokens=5000)\n",
        "tokenizer.fit_on_texts(texts=X_train)\n",
        "VOCAB_SIZE = len(tokenizer)\n",
        "print (tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVgBSnFTfE5D",
        "outputId": "c94e167c-b5ac-4693-8959-636ea9b13f9d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('<PAD>', 0), ('<UNK>', 1), ('39', 2), ('b', 3), ('gt', 4)]\n",
            "least freq token's freq: 14\n"
          ]
        }
      ],
      "source": [
        "# Sample of tokens\n",
        "print (take(5, tokenizer.token_to_index.items()))\n",
        "print (f\"least freq token's freq: {tokenizer.min_token_freq}\") # use this to adjust num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcscM_vL8KvP",
        "outputId": "1f88542e-e842-4384-87dd-0ade280d09e9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text to indices:\n",
            "  (preprocessed) â†’ china battles north korea nuclear talks\n",
            "  (tokenized) â†’ [  16 1491  285  142  114   24]\n"
          ]
        }
      ],
      "source": [
        "# Convert texts to sequences of indices\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_test = tokenizer.texts_to_sequences(X_test)\n",
        "preprocessed_text = tokenizer.sequences_to_texts([X_train[0]])[0]\n",
        "print (\"Text to indices:\\n\"\n",
        "    f\"  (preprocessed) â†’ {preprocessed_text}\\n\"\n",
        "    f\"  (tokenized) â†’ {X_train[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o91b6kKHjN4"
      },
      "source": [
        "## Padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD6CuPxOHlsw"
      },
      "source": [
        "We'll need to do 2D padding to our tokenized text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "dcNEooUHHkwn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def pad_sequences(sequences, max_seq_len=0):\n",
        "    \"\"\"Pad sequences to max length in sequence.\"\"\"\n",
        "    max_seq_len = max(max_seq_len, max(len(sequence) for sequence in sequences))\n",
        "    padded_sequences = np.zeros((len(sequences), max_seq_len))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        padded_sequences[i][:len(sequence)] = sequence\n",
        "    return padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfWHamHZHtT-",
        "outputId": "6042cabf-16d5-458a-9311-83c6e63b9c79",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3, 6)\n",
            "[[1.600e+01 1.491e+03 2.850e+02 1.420e+02 1.140e+02 2.400e+01]\n",
            " [1.445e+03 2.300e+01 6.560e+02 2.197e+03 1.000e+00 0.000e+00]\n",
            " [1.200e+02 1.400e+01 1.955e+03 1.005e+03 1.529e+03 4.014e+03]]\n"
          ]
        }
      ],
      "source": [
        "# 2D sequences\n",
        "padded = pad_sequences(X_train[0:3])\n",
        "print (padded.shape)\n",
        "print (padded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoWQk0hO9bK2"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSPbGwOkHvfj"
      },
      "source": [
        "We're going to create Datasets and DataLoaders to be able to efficiently create batches with our data splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "aSdfc0-cb9fc",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, y,):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"<Dataset(N={len(self)})>\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.X[index]\n",
        "        y = self.y[index]\n",
        "        return [X, len(X), y]\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        \"\"\"Processing on a batch.\"\"\"\n",
        "        # Get inputs\n",
        "        batch = np.array(batch, dtype=object)\n",
        "        X = batch[:, 0]\n",
        "        seq_lens = batch[:, 1]\n",
        "        y = np.stack(batch[:, 2], axis=0)\n",
        "\n",
        "        # Pad inputs\n",
        "        X = pad_sequences(sequences=X)\n",
        "\n",
        "        # Cast\n",
        "        X = torch.LongTensor(X.astype(np.int32))\n",
        "        seq_lens = torch.LongTensor(seq_lens.astype(np.int32))\n",
        "        y = torch.LongTensor(y.astype(np.int32))\n",
        "\n",
        "        return X, seq_lens, y\n",
        "\n",
        "    def create_dataloader(self, batch_size, shuffle=False, drop_last=False):\n",
        "        return torch.utils.data.DataLoader(\n",
        "            dataset=self, batch_size=batch_size, collate_fn=self.collate_fn,\n",
        "            shuffle=shuffle, drop_last=drop_last, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYZtzbWMIXY7",
        "outputId": "e41a9560-e8f4-49ff-dcf9-4a3b35def70f",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets:\n",
            "  Train dataset:<Dataset(N=84000)>\n",
            "  Val dataset: <Dataset(N=18000)>\n",
            "  Test dataset: <Dataset(N=18000)>\n",
            "Sample point:\n",
            "  X: [  16 1491  285  142  114   24]\n",
            "  seq_len: 6\n",
            "  y: 3\n"
          ]
        }
      ],
      "source": [
        "# Create datasets\n",
        "train_dataset = Dataset(X=X_train, y=y_train)\n",
        "val_dataset = Dataset(X=X_val, y=y_val)\n",
        "test_dataset = Dataset(X=X_test, y=y_test)\n",
        "print (\"Datasets:\\n\"\n",
        "    f\"  Train dataset:{train_dataset.__str__()}\\n\"\n",
        "    f\"  Val dataset: {val_dataset.__str__()}\\n\"\n",
        "    f\"  Test dataset: {test_dataset.__str__()}\\n\"\n",
        "    \"Sample point:\\n\"\n",
        "    f\"  X: {train_dataset[0][0]}\\n\"\n",
        "    f\"  seq_len: {train_dataset[0][1]}\\n\"\n",
        "    f\"  y: {train_dataset[0][2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1w6wVKJe9fxk",
        "outputId": "d4eeb474-e505-4033-9dc7-dd2978f3075d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample batch:\n",
            "  X: [64, 14]\n",
            "  seq_lens: [64]\n",
            "  y: [64]\n",
            "Sample point:\n",
            "  X: tensor([  16, 1491,  285,  142,  114,   24,    0,    0,    0,    0,    0,    0,\n",
            "           0,    0], device=\"cpu\")\n",
            " seq_len: 6\n",
            "  y: 3\n"
          ]
        }
      ],
      "source": [
        "# Create dataloaders\n",
        "batch_size = 64\n",
        "train_dataloader = train_dataset.create_dataloader(\n",
        "    batch_size=batch_size)\n",
        "val_dataloader = val_dataset.create_dataloader(\n",
        "    batch_size=batch_size)\n",
        "test_dataloader = test_dataset.create_dataloader(\n",
        "    batch_size=batch_size)\n",
        "batch_X, batch_seq_lens, batch_y = next(iter(train_dataloader))\n",
        "print (\"Sample batch:\\n\"\n",
        "    f\"  X: {list(batch_X.size())}\\n\"\n",
        "    f\"  seq_lens: {list(batch_seq_lens.size())}\\n\"\n",
        "    f\"  y: {list(batch_y.size())}\\n\"\n",
        "    \"Sample point:\\n\"\n",
        "    f\"  X: {batch_X[0]}\\n\"\n",
        "    f\" seq_len: {batch_seq_lens[0]}\\n\"\n",
        "    f\"  y: {batch_y[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB-g9F1TJOPx"
      },
      "source": [
        "## Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL5FcX0hJR37"
      },
      "source": [
        "Let's create the `Trainer` class that we'll use to facilitate training for our experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "k5KWF-DXJN2a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, device, loss_fn=None, optimizer=None, scheduler=None):\n",
        "\n",
        "        # Set params\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "        self.loss_fn = loss_fn\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "\n",
        "    def train_step(self, dataloader):\n",
        "        \"\"\"Train step.\"\"\"\n",
        "        # Set model to train mode\n",
        "        self.model.train()\n",
        "        loss = 0.0\n",
        "\n",
        "        # Iterate over train batches\n",
        "        for i, batch in enumerate(dataloader):\n",
        "\n",
        "            # Step\n",
        "            batch = [item.to(self.device) for item in batch]  # Set device\n",
        "            inputs, targets = batch[:-1], batch[-1]\n",
        "            self.optimizer.zero_grad()  # Reset gradients\n",
        "            z = self.model(inputs)  # Forward pass\n",
        "            J = self.loss_fn(z, targets)  # Define loss\n",
        "            J.backward()  # Backward pass\n",
        "            self.optimizer.step()  # Update weights\n",
        "\n",
        "            # Cumulative Metrics\n",
        "            loss += (J.detach().item() - loss) / (i + 1)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def eval_step(self, dataloader):\n",
        "        \"\"\"Validation or test step.\"\"\"\n",
        "        # Set model to eval mode\n",
        "        self.model.eval()\n",
        "        loss = 0.0\n",
        "        y_trues, y_probs = [], []\n",
        "\n",
        "        # Iterate over val batches\n",
        "        with torch.inference_mode():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "\n",
        "                # Step\n",
        "                batch = [item.to(self.device) for item in batch]  # Set device\n",
        "                inputs, y_true = batch[:-1], batch[-1]\n",
        "                z = self.model(inputs)  # Forward pass\n",
        "                J = self.loss_fn(z, y_true).item()\n",
        "\n",
        "                # Cumulative Metrics\n",
        "                loss += (J - loss) / (i + 1)\n",
        "\n",
        "                # Store outputs\n",
        "                y_prob = F.softmax(z).cpu().numpy()\n",
        "                y_probs.extend(y_prob)\n",
        "                y_trues.extend(y_true.cpu().numpy())\n",
        "\n",
        "        return loss, np.vstack(y_trues), np.vstack(y_probs)\n",
        "\n",
        "    def predict_step(self, dataloader):\n",
        "        \"\"\"Prediction step.\"\"\"\n",
        "        # Set model to eval mode\n",
        "        self.model.eval()\n",
        "        y_probs = []\n",
        "\n",
        "        # Iterate over val batches\n",
        "        with torch.inference_mode():\n",
        "            for i, batch in enumerate(dataloader):\n",
        "\n",
        "                # Forward pass w/ inputs\n",
        "                inputs, targets = batch[:-1], batch[-1]\n",
        "                z = self.model(inputs)\n",
        "\n",
        "                # Store outputs\n",
        "                y_prob = F.softmax(z).cpu().numpy()\n",
        "                y_probs.extend(y_prob)\n",
        "\n",
        "        return np.vstack(y_probs)\n",
        "    \n",
        "    def train(self, num_epochs, patience, train_dataloader, val_dataloader):\n",
        "        best_val_loss = np.inf\n",
        "        for epoch in range(num_epochs):\n",
        "            # Steps\n",
        "            train_loss = self.train_step(dataloader=train_dataloader)\n",
        "            val_loss, _, _ = self.eval_step(dataloader=val_dataloader)\n",
        "            self.scheduler.step(val_loss)\n",
        "\n",
        "            # Early stopping\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model = self.model\n",
        "                _patience = patience  # reset _patience\n",
        "            else:\n",
        "                _patience -= 1\n",
        "            if not _patience:  # 0\n",
        "                print(\"Stopping early!\")\n",
        "                break\n",
        "\n",
        "            # Logging\n",
        "            print(\n",
        "                f\"Epoch: {epoch+1} | \"\n",
        "                f\"train_loss: {train_loss:.5f}, \"\n",
        "                f\"val_loss: {val_loss:.5f}, \"\n",
        "                f\"lr: {self.optimizer.param_groups[0]['lr']:.2E}, \"\n",
        "                f\"_patience: {_patience}\"\n",
        "            )\n",
        "        return best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUMibnFtuq_i"
      },
      "source": [
        "# Vanilla RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwyHfs4JvC7F"
      },
      "source": [
        "Inputs to RNNs are sequential like text or time-series."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "d3e6DbkwhXow",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOs64ORxvBei",
        "outputId": "5777e2f7-adc9-4306-cb2c-383da7e851d6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8, 100])\n",
            "torch.Size([1, 64])\n"
          ]
        }
      ],
      "source": [
        "# Input\n",
        "sequence_size = 8 # words per input\n",
        "x = torch.rand((BATCH_SIZE, sequence_size, EMBEDDING_DIM))\n",
        "seq_lens = torch.randint(high=sequence_size, size=(1, BATCH_SIZE))\n",
        "print (x.shape)\n",
        "print (seq_lens.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufxzi5G0DScl"
      },
      "source": [
        "<div align=\"left\">\n",
        "<img src=\"https://madewithml.com/static/images/foundations/rnn/vanilla.png\" width=\"500\">\n",
        "</div>\n",
        "\n",
        "RNN forward pass for a single time step $X_t$:\n",
        "\n",
        "$h_t = tanh(W_{hh}h_{t-1} + W_{xh}X_t+b_h)$\n",
        "\n",
        "*where*:\n",
        "* $W_{hh}$ = hidden units weights| $\\in \\mathbb{R}^{HXH}$ ($H$ is the hidden dim)\n",
        "* $h_{t-1}$ = previous timestep's hidden state $\\in \\mathbb{R}^{NXH}$\n",
        "* $W_{xh}$ = input weights| $\\in \\mathbb{R}^{EXH}$\n",
        "* $X_t$ = input at time step t | $\\in \\mathbb{R}^{NXE}$ ($N$ is the batch size, $E$ is the embedding dim)\n",
        "* $b_h$ = hidden units bias $\\in \\mathbb{R}^{HX1}$\n",
        "* $h_t$ = output from RNN for timestep $t$\n",
        "\n",
        "> At the first time step, the previous hidden state $h_{t-1}$ can either be a zero vector (unconditioned) or initialized (conditioned). If we are conditioning the RNN, the first hidden state $h_0$ can belong to a specific condition or we can concat the specific condition to the randomly initialized hidden vectors at each time step. More on this in the subsequent notebooks on RNNs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "K-Eb1HdRhnmu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "RNN_HIDDEN_DIM = 128\n",
        "DROPOUT_P = 0.1\n",
        "RNN_DROPOUT_P = 0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJmu7XUTsc3F",
        "outputId": "e3145a99-6253-4bc2-d0c1-4866a2df7a09",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 128])\n"
          ]
        }
      ],
      "source": [
        "# Initialize hidden state\n",
        "hidden_t = torch.zeros((BATCH_SIZE, RNN_HIDDEN_DIM))\n",
        "print (hidden_t.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ujvpD062Fi6"
      },
      "source": [
        "We'll show how to create an RNN cell using PyTorch's [`RNNCell`](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html#torch.nn.RNNCell) and the more abstracted [`RNN`](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-O2Du78ut8J",
        "outputId": "25942087-9be7-43e3-f587-d82ae04d83f7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNNCell(100, 128)\n"
          ]
        }
      ],
      "source": [
        "# Initialize RNN cell\n",
        "rnn_cell = nn.RNNCell(EMBEDDING_DIM, RNN_HIDDEN_DIM)\n",
        "print (rnn_cell)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqOtTdDx3Nz7",
        "outputId": "7bbb3d02-2b89-44b2-97ef-3d31cf4b11d8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8, 128])\n"
          ]
        }
      ],
      "source": [
        "# Forward pass through RNN\n",
        "x = x.permute(1, 0, 2) # RNN needs batch_size to be at dim 1\n",
        "\n",
        "# Loop through the inputs time steps\n",
        "hiddens = []\n",
        "for t in range(sequence_size):\n",
        "    hidden_t = rnn_cell(x[t], hidden_t)\n",
        "    hiddens.append(hidden_t)\n",
        "hiddens = torch.stack(hiddens)\n",
        "hiddens = hiddens.permute(1, 0, 2) # bring batch_size back to dim 0\n",
        "print (hiddens.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1rb4lsltO-c",
        "outputId": "707bac3b-5613-48f1-ce7d-6b593a2927c1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out:  torch.Size([64, 8, 128])\n",
            "h_n:  torch.Size([1, 64, 128])\n"
          ]
        }
      ],
      "source": [
        "# We also could've used a more abstracted layer\n",
        "x = torch.rand((BATCH_SIZE, sequence_size, EMBEDDING_DIM))\n",
        "rnn = nn.RNN(EMBEDDING_DIM, RNN_HIDDEN_DIM, batch_first=True)\n",
        "out, h_n = rnn(x) # h_n is the last hidden state\n",
        "print (\"out: \", out.shape)\n",
        "print (\"h_n: \", h_n.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYoA2Wbet9wr",
        "outputId": "2d97c9d9-fda5-4565-b85a-4e48600408f4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.5056,  0.6157,  0.4275,  ...,  0.1804, -0.1480, -0.4822],\n",
            "        [-0.1490,  0.6549,  0.3184,  ...,  0.2831, -0.3557, -0.5438],\n",
            "        [-0.5290,  0.4321,  0.0885,  ...,  0.4848, -0.2672, -0.2660],\n",
            "        ...,\n",
            "        [-0.3273,  0.6155, -0.2170,  ...,  0.1718, -0.1623, -0.3876],\n",
            "        [-0.3860,  0.3749,  0.0142,  ...,  0.6179, -0.3790, -0.2459],\n",
            "        [-0.0464,  0.4893, -0.2189,  ...,  0.3532, -0.3793, -0.5216]],\n",
            "       grad_fn=<SliceBackward>)\n",
            "tensor([[-0.5056,  0.6157,  0.4275,  ...,  0.1804, -0.1480, -0.4822],\n",
            "        [-0.1490,  0.6549,  0.3184,  ...,  0.2831, -0.3557, -0.5438],\n",
            "        [-0.5290,  0.4321,  0.0885,  ...,  0.4848, -0.2672, -0.2660],\n",
            "        ...,\n",
            "        [-0.3273,  0.6155, -0.2170,  ...,  0.1718, -0.1623, -0.3876],\n",
            "        [-0.3860,  0.3749,  0.0142,  ...,  0.6179, -0.3790, -0.2459],\n",
            "        [-0.0464,  0.4893, -0.2189,  ...,  0.3532, -0.3793, -0.5216]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# The same tensors\n",
        "print (out[:,-1,:])\n",
        "print (h_n.squeeze(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WJ_ZqzAu6Uk"
      },
      "source": [
        "In our model, we want to use the RNN's output after the last relevant token in the sentence is processed. The last relevant token doesn't refer the `<PAD>` tokens but to the last actual word in the sentence and its index is different for each input in the batch. This is why we included a `seq_lens` tensor in our batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PqPOY6qbu6e0",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def gather_last_relevant_hidden(hiddens, seq_lens):\n",
        "    \"\"\"Extract and collect the last relevant \n",
        "    hidden state based on the sequence length.\"\"\"\n",
        "    seq_lens = seq_lens.long().detach().cpu().numpy() - 1\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(seq_lens):\n",
        "        out.append(hiddens[batch_index, column_index])\n",
        "    return torch.stack(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDIwDHJOu7Kj",
        "outputId": "5aca18f3-d48a-44d4-a981-64a05c0d9118",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([64, 128])"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the last relevant hidden state\n",
        "gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens).squeeze(0).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnUh3iRpJ2j8"
      },
      "source": [
        "There are many different ways to use RNNs. So far we've processed our inputs one timestep at a time and we could either use the RNN's output at each time step or just use the final input timestep's RNN output. Let's look at a few other possibilities.\n",
        "\n",
        "<div align=\"left\">\n",
        "<img src=\"https://madewithml.com/static/images/foundations/rnn/architectures.png\" width=\"1000\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfhjWZRD94hK"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "OXDA4UNdxmZT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "qoBC0u8XpbVv",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "HIDDEN_DIM = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "UPP5ROd69mXC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
        "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
        "        super(RNN, self).__init__()\n",
        "        \n",
        "        # Initialize embeddings\n",
        "        self.embeddings = nn.Embedding(\n",
        "            embedding_dim=embedding_dim, num_embeddings=vocab_size,\n",
        "            padding_idx=padding_idx)\n",
        "        \n",
        "        # RNN\n",
        "        self.rnn = nn.RNN(embedding_dim, rnn_hidden_dim, batch_first=True)\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Embed\n",
        "        x_in, seq_lens = inputs\n",
        "        x_in = self.embeddings(x_in)\n",
        "            \n",
        "        # Rnn outputs\n",
        "        out, h_n = self.rnn(x_in)\n",
        "        z = gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens)\n",
        "\n",
        "        # FC layers\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        z = self.fc2(z)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LH_z2FqUn5DR",
        "outputId": "1c658926-7335-4350-b48e-8b5a207cdeb4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of RNN(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (rnn): RNN(100, 128, batch_first=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=128, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ]
        }
      ],
      "source": [
        "# Simple RNN cell\n",
        "model = RNN(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM, \n",
        "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
        "model = model.to(device) # set device\n",
        "print (model.named_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9ggYO6yHIm2"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "geKOPVzVK6S9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "OfSBvenCiIce",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "NUM_LAYERS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "PATIENCE = 10\n",
        "NUM_EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "4ilmlIMUzNIj",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define Loss\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Ucn3tYq1_sE1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE) \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.1, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "X5ac-uJXb-F7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Trainer module\n",
        "trainer = Trainer(\n",
        "    model=model, device=device, loss_fn=loss_fn, \n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-uKcZrABKVZ",
        "outputId": "b5f137dd-87ed-4a26-a678-c67a3afed627",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 1.25500, val_loss: 1.12003, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 2 | train_loss: 1.03130, val_loss: 0.97659, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 3 | train_loss: 0.89955, val_loss: 0.87245, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 4 | train_loss: 0.79928, val_loss: 0.79484, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 5 | train_loss: 0.72322, val_loss: 0.73841, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 6 | train_loss: 0.66636, val_loss: 0.69743, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 7 | train_loss: 0.62149, val_loss: 0.66580, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 8 | train_loss: 0.58615, val_loss: 0.64251, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 9 | train_loss: 0.55711, val_loss: 0.62356, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 10 | train_loss: 0.53208, val_loss: 0.60801, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 11 | train_loss: 0.51060, val_loss: 0.59539, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 12 | train_loss: 0.49146, val_loss: 0.58582, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 13 | train_loss: 0.47394, val_loss: 0.57700, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 14 | train_loss: 0.45890, val_loss: 0.57153, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 15 | train_loss: 0.44470, val_loss: 0.56508, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 16 | train_loss: 0.43173, val_loss: 0.56183, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 17 | train_loss: 0.41952, val_loss: 0.55895, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 18 | train_loss: 0.40817, val_loss: 0.55745, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 19 | train_loss: 0.39774, val_loss: 0.55570, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 20 | train_loss: 0.38761, val_loss: 0.55598, lr: 1.00E-04, _patience: 9\n",
            "Epoch: 21 | train_loss: 0.37759, val_loss: 0.55710, lr: 1.00E-04, _patience: 8\n",
            "Epoch: 22 | train_loss: 0.36872, val_loss: 0.55861, lr: 1.00E-04, _patience: 7\n",
            "Epoch: 23 | train_loss: 0.35990, val_loss: 0.55922, lr: 1.00E-05, _patience: 6\n",
            "Epoch: 24 | train_loss: 0.33898, val_loss: 0.53767, lr: 1.00E-05, _patience: 10\n",
            "Epoch: 25 | train_loss: 0.33417, val_loss: 0.53845, lr: 1.00E-05, _patience: 9\n",
            "Epoch: 26 | train_loss: 0.33196, val_loss: 0.53936, lr: 1.00E-05, _patience: 8\n",
            "Epoch: 27 | train_loss: 0.33048, val_loss: 0.54046, lr: 1.00E-05, _patience: 7\n",
            "Epoch: 28 | train_loss: 0.32941, val_loss: 0.54174, lr: 1.00E-06, _patience: 6\n",
            "Epoch: 29 | train_loss: 0.32617, val_loss: 0.53706, lr: 1.00E-06, _patience: 10\n",
            "Epoch: 30 | train_loss: 0.32539, val_loss: 0.53703, lr: 1.00E-06, _patience: 10\n",
            "Epoch: 31 | train_loss: 0.32537, val_loss: 0.53706, lr: 1.00E-06, _patience: 9\n",
            "Epoch: 32 | train_loss: 0.32483, val_loss: 0.53709, lr: 1.00E-06, _patience: 8\n",
            "Epoch: 33 | train_loss: 0.32509, val_loss: 0.53717, lr: 1.00E-07, _patience: 7\n",
            "Epoch: 34 | train_loss: 0.32449, val_loss: 0.53715, lr: 1.00E-07, _patience: 6\n",
            "Epoch: 35 | train_loss: 0.32475, val_loss: 0.53718, lr: 1.00E-07, _patience: 5\n",
            "Epoch: 36 | train_loss: 0.32441, val_loss: 0.53720, lr: 1.00E-07, _patience: 4\n",
            "Epoch: 37 | train_loss: 0.32452, val_loss: 0.53722, lr: 1.00E-08, _patience: 3\n",
            "Epoch: 38 | train_loss: 0.32467, val_loss: 0.53722, lr: 1.00E-08, _patience: 2\n",
            "Epoch: 39 | train_loss: 0.32426, val_loss: 0.53723, lr: 1.00E-08, _patience: 1\n",
            "Stopping early!\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "best_model = trainer.train(\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axytMjGQmz6T"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "-Ly3zjVAm4Wr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ojOfz9rjm4ZI",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_performance(y_true, y_pred, classes):\n",
        "    \"\"\"Per-class performance metrics.\"\"\"\n",
        "    # Performance\n",
        "    performance = {\"overall\": {}, \"class\": {}}\n",
        "\n",
        "    # Overall performance\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
        "    performance[\"overall\"][\"precision\"] = metrics[0]\n",
        "    performance[\"overall\"][\"recall\"] = metrics[1]\n",
        "    performance[\"overall\"][\"f1\"] = metrics[2]\n",
        "    performance[\"overall\"][\"num_samples\"] = np.float64(len(y_true))\n",
        "\n",
        "    # Per-class performance\n",
        "    metrics = precision_recall_fscore_support(y_true, y_pred, average=None)\n",
        "    for i in range(len(classes)):\n",
        "        performance[\"class\"][classes[i]] = {\n",
        "            \"precision\": metrics[0][i],\n",
        "            \"recall\": metrics[1][i],\n",
        "            \"f1\": metrics[2][i],\n",
        "            \"num_samples\": np.float64(metrics[3][i]),\n",
        "        }\n",
        "\n",
        "    return performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tho3e_yQzqZ1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fylHduxTK-0N",
        "outputId": "51c33d82-7b0b-4b24-bbf6-b4dab8084dd1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.8211429212272771,\n",
            "  \"recall\": 0.8212777777777778,\n",
            "  \"f1\": 0.8208202838924475,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Determine performance\n",
        "performance = get_performance(\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
        "print (json.dumps(performance[\"overall\"], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jR3trOIQcWY2"
      },
      "source": [
        "# Gated RNNs: LSTMs & GRUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqFsufNVch4W"
      },
      "source": [
        "While our simple RNNs so far are great for sequentially processing our inputs, they have quite a few disadvantages. They commonly suffer from exploding or vanishing gradients as a result using the same set of weights ($W_{xh}$ and $W_{hh}$) with each timestep's input. During backpropagation, this can cause gradients to explode (>1) or vanish (<1). If you multiply any number greater than 1 with itself over and over, it moves towards infinity (exploding gradients) and similarly,  If you multiply any number less than 1 with itself over and over, it moves towards zero (vanishing gradients). To mitigate this issue, gated RNNs were devised to selectively retrain information. If you're interested in learning more of the specifics, this [post](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is a must-read.\n",
        "\n",
        "There are two popular types of gated RNNs: Long Short-term Memory (LSTMs) units and Gated Recurrent Units (GRUs).\n",
        "\n",
        "<div align=\"left\">\n",
        "<img src=\"https://madewithml.com/static/images/foundations/rnn/gated.png\" width=\"600\">\n",
        "</div>\n",
        "<a href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\">Understanding LSTM Networks</a> - Chris Olah\n",
        "\n",
        "\n",
        "> When deciding between LSTMs and GRUs, empirical performance is the best factor but in genreal GRUs offer similar performance with less complexity (less weights). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXuW93ef7xcg",
        "outputId": "fced8e40-7714-4dc3-dd4f-c3962107e61d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 8, 100])\n"
          ]
        }
      ],
      "source": [
        "# Input\n",
        "sequence_size = 8 # words per input\n",
        "x = torch.rand((BATCH_SIZE, sequence_size, EMBEDDING_DIM))\n",
        "print (x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "PqVCqWv2fz4N",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# GRU \n",
        "gru = nn.GRU(input_size=EMBEDDING_DIM, hidden_size=RNN_HIDDEN_DIM, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-Va8pRB9Xn4",
        "outputId": "34bd4a8e-a167-48c0-d47c-25bb65459698",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out: torch.Size([64, 8, 128])\n",
            "h_n: torch.Size([1, 64, 128])\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "out, h_n = gru(x)\n",
        "print (f\"out: {out.shape}\")\n",
        "print (f\"h_n: {h_n.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N9Ga-Jx874z"
      },
      "source": [
        "## Bidirectional RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3HkL93688AB"
      },
      "source": [
        "We can also have RNNs that process inputs from both directions (first token to last token and vice versa) and combine their outputs. This architecture is known as a bidirectional RNN. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ne711WL888Qf",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# GRU \n",
        "gru = nn.GRU(input_size=EMBEDDING_DIM, hidden_size=RNN_HIDDEN_DIM, \n",
        "             batch_first=True, bidirectional=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52sSVNd_9TwH",
        "outputId": "dac7bca7-1ada-4878-f62a-ecbec58c5014",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "out: torch.Size([64, 8, 256])\n",
            "h_n: torch.Size([2, 64, 128])\n"
          ]
        }
      ],
      "source": [
        "# Forward pass\n",
        "out, h_n = gru(x)\n",
        "print (f\"out: {out.shape}\")\n",
        "print (f\"h_n: {h_n.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lO7l6idz9wMx"
      },
      "source": [
        "Notice that the output for each sample at each timestamp has size 256 (double the `RNN_HIDDEN_DIM`). This is because this includes both the forward and backward directions from the BiRNN. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZUXGLRs981z"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "8I-AEQmG-Ge-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class GRU(nn.Module):\n",
        "    def __init__(self, embedding_dim, vocab_size, rnn_hidden_dim,\n",
        "                 hidden_dim, dropout_p, num_classes, padding_idx=0):\n",
        "        super(GRU, self).__init__()\n",
        "        \n",
        "        # Initialize embeddings\n",
        "        self.embeddings = nn.Embedding(embedding_dim=embedding_dim,\n",
        "                                       num_embeddings=vocab_size,\n",
        "                                       padding_idx=padding_idx)\n",
        "        \n",
        "        # RNN\n",
        "        self.rnn = nn.GRU(embedding_dim, rnn_hidden_dim, \n",
        "                          batch_first=True, bidirectional=True)\n",
        "     \n",
        "        # FC weights\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_dim*2, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Embed\n",
        "        x_in, seq_lens = inputs\n",
        "        x_in = self.embeddings(x_in)\n",
        "            \n",
        "        # Rnn outputs\n",
        "        out, h_n = self.rnn(x_in)\n",
        "        z = gather_last_relevant_hidden(hiddens=out, seq_lens=seq_lens)\n",
        "\n",
        "        # FC layers\n",
        "        z = self.fc1(z)\n",
        "        z = self.dropout(z)\n",
        "        z = self.fc2(z)\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-tSbP86-e_s",
        "outputId": "b618bd99-6e30-496c-bb8e-de78c5e7e83a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method Module.named_parameters of GRU(\n",
            "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
            "  (rnn): GRU(100, 128, batch_first=True, bidirectional=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            "  (fc1): Linear(in_features=256, out_features=100, bias=True)\n",
            "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
            ")>\n"
          ]
        }
      ],
      "source": [
        "# Simple RNN cell\n",
        "model = GRU(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM, \n",
        "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
        "model = model.to(device) # set device\n",
        "print (model.named_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biToCtZw-oPq"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "LByz0XL1-pT1",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define Loss\n",
        "class_weights_tensor = torch.Tensor(list(class_weights.values())).to(device)\n",
        "loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "PMUYuLaI-shJ",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define optimizer & scheduler\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE) \n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", factor=0.1, patience=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "G3jTgXy4-wDw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Trainer module\n",
        "trainer = Trainer(\n",
        "    model=model, device=device, loss_fn=loss_fn, \n",
        "    optimizer=optimizer, scheduler=scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pH_qGGHq-0K5",
        "outputId": "b338ddbf-39ab-41ae-eb4a-4c193fd293ea",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 1.16930, val_loss: 0.94210, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 2 | train_loss: 0.82127, val_loss: 0.72819, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 3 | train_loss: 0.65862, val_loss: 0.64288, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 4 | train_loss: 0.58088, val_loss: 0.60078, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 5 | train_loss: 0.53120, val_loss: 0.57543, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 6 | train_loss: 0.49535, val_loss: 0.55925, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 7 | train_loss: 0.46603, val_loss: 0.54962, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 8 | train_loss: 0.44215, val_loss: 0.54387, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 9 | train_loss: 0.42093, val_loss: 0.53825, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 10 | train_loss: 0.40259, val_loss: 0.53758, lr: 1.00E-04, _patience: 10\n",
            "Epoch: 11 | train_loss: 0.38635, val_loss: 0.53765, lr: 1.00E-04, _patience: 9\n",
            "Epoch: 12 | train_loss: 0.37150, val_loss: 0.53935, lr: 1.00E-04, _patience: 8\n",
            "Epoch: 13 | train_loss: 0.35735, val_loss: 0.54147, lr: 1.00E-04, _patience: 7\n",
            "Epoch: 14 | train_loss: 0.34519, val_loss: 0.54368, lr: 1.00E-05, _patience: 6\n",
            "Epoch: 15 | train_loss: 0.31900, val_loss: 0.53039, lr: 1.00E-05, _patience: 10\n",
            "Epoch: 16 | train_loss: 0.31445, val_loss: 0.53071, lr: 1.00E-05, _patience: 9\n",
            "Epoch: 17 | train_loss: 0.31230, val_loss: 0.53166, lr: 1.00E-05, _patience: 8\n",
            "Epoch: 18 | train_loss: 0.31042, val_loss: 0.53303, lr: 1.00E-05, _patience: 7\n",
            "Epoch: 19 | train_loss: 0.30883, val_loss: 0.53416, lr: 1.00E-06, _patience: 6\n",
            "Epoch: 20 | train_loss: 0.30481, val_loss: 0.53245, lr: 1.00E-06, _patience: 5\n",
            "Epoch: 21 | train_loss: 0.30435, val_loss: 0.53248, lr: 1.00E-06, _patience: 4\n",
            "Epoch: 22 | train_loss: 0.30458, val_loss: 0.53250, lr: 1.00E-06, _patience: 3\n",
            "Epoch: 23 | train_loss: 0.30449, val_loss: 0.53258, lr: 1.00E-07, _patience: 2\n",
            "Epoch: 24 | train_loss: 0.30425, val_loss: 0.53252, lr: 1.00E-07, _patience: 1\n",
            "Stopping early!\n"
          ]
        }
      ],
      "source": [
        "# Train\n",
        "best_model = trainer.train(\n",
        "    NUM_EPOCHS, PATIENCE, train_dataloader, val_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vskwiiI3V3S6"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "4OQ5pEVco3xb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Itq7lT9qV9Y8",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "test_loss, y_true, y_prob = trainer.eval_step(dataloader=test_dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VT9C8rUyWAWF",
        "outputId": "27416cef-81fb-4ff3-9d57-e1ce747cafba",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"precision\": 0.8206922733621068,\n",
            "  \"recall\": 0.8209444444444445,\n",
            "  \"f1\": 0.8205583307860935,\n",
            "  \"num_samples\": 18000.0\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Determine performance\n",
        "performance = get_performance(\n",
        "    y_true=y_test, y_pred=y_pred, classes=label_encoder.classes)\n",
        "print (json.dumps(performance[\"overall\"], indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "mbxUAecEo-QC",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Save artifacts\n",
        "dir = Path(\"gru\")\n",
        "dir.mkdir(parents=True, exist_ok=True)\n",
        "label_encoder.save(fp=Path(dir, \"label_encoder.json\"))\n",
        "tokenizer.save(fp=Path(dir, 'tokenizer.json'))\n",
        "torch.save(best_model.state_dict(), Path(dir, \"model.pt\"))\n",
        "with open(Path(dir, 'performance.json'), \"w\") as fp:\n",
        "    json.dump(performance, indent=2, sort_keys=False, fp=fp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yeiD1T_QZpdk"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "OAyVsyDepD5-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_probability_distribution(y_prob, classes):\n",
        "    \"\"\"Create a dict of class probabilities from an array.\"\"\"\n",
        "    results = {}\n",
        "    for i, class_ in enumerate(classes):\n",
        "        results[class_] = np.float64(y_prob[i])\n",
        "    sorted_results = {k: v for k, v in sorted(\n",
        "        results.items(), key=lambda item: item[1], reverse=True)}\n",
        "    return sorted_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNxo3XPSpLgG",
        "outputId": "aa24d9bc-e564-4969-81fb-28eb6f8db994",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GRU(\n",
              "  (embeddings): Embedding(5000, 100, padding_idx=0)\n",
              "  (rnn): GRU(100, 128, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (fc1): Linear(in_features=256, out_features=100, bias=True)\n",
              "  (fc2): Linear(in_features=100, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load artifacts\n",
        "device = torch.device(\"cpu\")\n",
        "label_encoder = LabelEncoder.load(fp=Path(dir, \"label_encoder.json\"))\n",
        "tokenizer = Tokenizer.load(fp=Path(dir, 'tokenizer.json'))\n",
        "model = GRU(\n",
        "    embedding_dim=EMBEDDING_DIM, vocab_size=VOCAB_SIZE, \n",
        "    rnn_hidden_dim=RNN_HIDDEN_DIM, hidden_dim=HIDDEN_DIM, \n",
        "    dropout_p=DROPOUT_P, num_classes=NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(Path(dir, \"model.pt\"), map_location=device))\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "50gWn5a5pQDK",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = Trainer(model=model, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvHtQ9S8pRQX",
        "outputId": "befbfae8-2700-4dcf-810c-e360845adda6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['final tennis tournament starts next week']\n"
          ]
        }
      ],
      "source": [
        "# Dataloader\n",
        "text = \"The final tennis tournament starts next week.\"\n",
        "X = tokenizer.texts_to_sequences([preprocess(text)])\n",
        "print (tokenizer.sequences_to_texts(X))\n",
        "y_filler = label_encoder.encode([label_encoder.classes[0]]*len(X))\n",
        "dataset = Dataset(X=X, y=y_filler)\n",
        "dataloader = dataset.create_dataloader(batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM03X5w9pRTM",
        "outputId": "67980f4d-2523-4608-b6f9-afc0586d7f9d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Sports']"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Inference\n",
        "y_prob = trainer.predict_step(dataloader)\n",
        "y_pred = np.argmax(y_prob, axis=1)\n",
        "label_encoder.decode(y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5omEa3BHpRVZ",
        "outputId": "8736cf05-10aa-4d0c-996e-5e1daadf5139",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"Sports\": 0.9944021105766296,\n",
            "  \"World\": 0.004813326057046652,\n",
            "  \"Sci/Tech\": 0.0007053817971609533,\n",
            "  \"Business\": 7.924934470793232e-05\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Class distributions\n",
        "prob_dist = get_probability_distribution(y_prob=y_prob[0], classes=label_encoder.classes)\n",
        "print (json.dumps(prob_dist, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xknbwa-bW3U"
      },
      "source": [
        "> We will learn how to create more context-aware representations and a little bit of interpretability with RNNs in the next lesson on <a target=\"_blank\" href=\"https://madewithml.com/courses/foundations/attention/\">attention</a>."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "13_Recurrent_Neural_Networks",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
